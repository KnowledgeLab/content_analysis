{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Classification\n",
    "\n",
    "This week, we shift from gathering human textual classifications through crowdsourcing, to instead using machine learning models and algorithms that train on those human classifications and extend them to documents far too numerous to read. If you recall, *clustering* allows us to stably partition text data (e.g., documents) according to all patterns of covariation among their features. *Classification*, by contrast, partitions text data according to only those features and their variation that enable us to mimic and extrapolate human annotations.\n",
    "\n",
    "In this notebook, we will show how to use a variety of classification methods, including Naïve Bayes, Logistic regression, K-nearest neighbor, decision trees and random forests, support vector machines and even a simple neural network, the perceptron. We will also demonstrate ensemble techniques that can link several such methods into a single, more accurate, classification pipeline. We will finally learn to use conventions and metrics to evaluate classifier performance on out-of-sample data. \n",
    "\n",
    "For this notebook we will be using the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "#For ML\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.decomposition\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.datasets import fetch_20newsgroups, make_blobs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  #Feature extraction\n",
    "from sklearn.naive_bayes import MultinomialNB #Our learner.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import neighbors\n",
    "\n",
    "import nltk #For tokenizing and normalizing\n",
    "import numpy as np #arrays\n",
    "import matplotlib.pyplot as plt #Plots\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn #Makes plots look nice, also heatmaps\n",
    "import scipy as sp #for interp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#These are from the standard library\n",
    "import collections\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "import pandas\n",
    "import requests\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also develop a couple of functions for splitting our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes from Scratch\n",
    "\n",
    "First, let's build a Naive Bayes classifier from scratch to illustrate what's in a classifier. This example is drawn from *Data Science from Scratch* by Joel Grus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Naïve Bayes contains an independence assumption that the probability of features conditional on classification $S$ is equivalent to the product of conditional probabilities associated with each of those features, hence this model's *naïvité*: $P(X_1 = x_1,\\dots,X_n = x_n\\,|\\,S) = P(X_1 = x_1\\,|\\,S)\\times \\dots \n",
    "    \\times P(X_n = x_n\\,|\\,S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be concrete, let's assume we are building a spam filter. \n",
    "\n",
    "Given a vocabulary $w_1,\\dots,w_n$, let $X_i$ be the event \"message contains\" $w_i$.\" $X_i = x_i, x_i \\in \\{0,1\\}$. \n",
    "\n",
    "$S$ is the event \"message is spam\" and $\\neg S$ is the event \"message is not spam.\"\n",
    "\n",
    "According to Bayes' Theorem: \n",
    "\n",
    "$P(S\\,|\\,X_1 = x_1,\\dots, X_n = x_n) = \\frac{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)P(S)}{P(X_1 = x_1,\\dots, X_n = x_n)} = \\frac{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)P(S)}{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)P(S)\\, + \\,P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,\\neg S)P(\\neg S)}$\n",
    "\n",
    "We further assume that we have no knowledge of the prior probability of spam; so $P(S) = P(\\neg S) = 0.5$ (this is the principle of indifference)\n",
    "\n",
    "With this simplification, $P(S\\,|\\,X_1 = x_1,\\dots, X_n = x_n) = \\frac{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)}{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)\\, +\\, P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,\\neg S)}$ \n",
    "\n",
    "Now we make the Naive Bayes assumption: $P(X_1 = x_1,\\dots,X_n = x_n\\,|\\,S) = P(X_1 = x_1\\,|\\,S)\\times \\dots \n",
    "    \\times P(X_n = x_n\\,|\\,S)$\n",
    "    \n",
    "We can estimate $P(X_i = x_i\\,|\\,S)$ by computing the fraction of spam messages containing the word $i$, e.g., Obamacare. \n",
    "\n",
    "Smoothing: $P(X_i\\,|\\,S) = \\frac{(k + \\textrm{number of spams containing}\\, w_i)}{(2k + \\textrm{number of spams})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, to code this up..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with spam detection, we have data from: https://spamassassin.apache.org/publiccorpus/ in `data/Spam_Data` with the spam in `data/Spam_Data/spam` and the non spam in `data/Spam_Data/not_spam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadEmailDir(targetDir, category):\n",
    "    allFileNames = os.listdir(targetDir)\n",
    "    #We need to make them into useable paths and filter out hidden files\n",
    "    filePaths = [os.path.join(targetDir, fname) for fname in allFileNames if fname[0] != '.']\n",
    "\n",
    "    # regex for stripping out the leading \"Subject:\" and any spaces after it\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "    \n",
    "    #The dict that will become the DataFrame\n",
    "    emailDict = {\n",
    "        'category' : [],\n",
    "        'text' : [],\n",
    "    }\n",
    "    for fPath in filePaths:\n",
    "        with open(fPath, errors='surrogateescape') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    #Could also save the subject field\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    emailDict['text'].append(subject)\n",
    "    emailDict['category'] = [category] * len(emailDict['text'])\n",
    "    return pandas.DataFrame(emailDict)\n",
    "\n",
    "spamDF = loadEmailDir('data/Spam_Data/spam', 'spam')\n",
    "spamDF = spamDF.append(loadEmailDir('data/Spam_Data/not_spam', 'not spam'), ignore_index= True)\n",
    "spamDF['is_spam'] = [c == 'spam' for c in spamDF['category']]\n",
    "spamDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tokenize and normalize like normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None, lemmer = None, vocab = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "\n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "\n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "        \n",
    "    #We will return a list with the stopwords removed\n",
    "    if vocab is not None:\n",
    "        vocab_str = '|'.join(vocab)\n",
    "        workingIter = (w for w in workingIter if re.match(vocab_str, w))\n",
    "    \n",
    "    return list(workingIter)\n",
    "\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "spamDF['tokenized_emails'] = spamDF['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "spamDF['normalized_emails'] = spamDF['tokenized_emails'].apply(lambda x: normlizeTokens(x, stopwordLst = None, stemmer = None))\n",
    "spamDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to count the number of times each word shows up in spam and non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(traingDF, textColumn, trainingColumn):\n",
    "    counts = collections.defaultdict(lambda: [0, 0])\n",
    "    for index, row in traingDF.iterrows():\n",
    "        for word in set(row[textColumn]):\n",
    "            if row[trainingColumn]:\n",
    "                counts[word][0] += 1\n",
    "            else:\n",
    "                counts[word][1] += 1\n",
    "    return counts\n",
    "wordCounts = count_words(spamDF, 'normalized_emails', 'is_spam')\n",
    "wordCounts['cancel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to convert these counts into (smoothed) probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets \n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    retTuples = []\n",
    "    for w, (spam, non_spam) in counts.items():\n",
    "        retTuples.append((w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k)))\n",
    "    return retTuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to come up with a way to compute the spam probability for a message, given word probabilities. With the Naive Bayes assumption, we *would* be multiplying together a bunch of probabilities. This is bad (e.g., floating point underflow) so we instead compute:\n",
    "\n",
    "$p_1 *\\dots*p_n = \\exp(\\, \\log(p_1) + \\dots + \\log(p_n)\\,)$; recall $\\log(ab) = \\log a + \\log b$ and $\\exp(\\, \\log x \\,) = x$\n",
    "\n",
    "Thank you, John Napier (1550-1617), for inventing logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message_words):\n",
    "    #message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0 #Initialize; we are working with log probs to deal with underflow.\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs: #We iterate over all possible words we've observed\n",
    "        # for each word in the message, \n",
    "        # add the log probability of seeing it \n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam) #This is prob of seeing word if spam\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam) #This is prob of seeing word if not spam\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "    P = 1/(1 + math.exp(log_prob_if_not_spam - log_prob_if_spam))\n",
    "    #prob_if_spam = math.exp(log_prob_if_spam) #Compute numerator\n",
    "    #prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    #return prob_if_spam / (prob_if_spam + prob_if_not_spam) #Compute whole thing and return\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think: how would this change if $P(S) \\neq P(\\neg S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a class (this is a Python term) for our Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = [] #Initializes word_probs as an empty list, sets a default smoothing parameters\n",
    "\n",
    "    def train(self, training_set, trainingColumn, textColumn): #Operates on the training_set\n",
    "    \n",
    "        # count spam and non-spam messages: first step of training\n",
    "        num_spams = training_set[trainingColumn].value_counts()[True]\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        \n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set, textColumn, trainingColumn)\n",
    "        self.word_probs = word_probabilities(word_counts, \n",
    "                                             num_spams, \n",
    "                                             num_non_spams,\n",
    "                                             self.k) #\"Train\" classifier \n",
    "                                             \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message) #Now we have all we need to classify a message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train and then evaluate our model, we will split our data into ***training*** & ***testing*** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "shuffledSpamDF = spamDF.reindex(np.random.permutation(spamDF.index))\n",
    "holdBackFraction = .2\n",
    "holdBackIndex = int(holdBackFraction * len(shuffledSpamDF))\n",
    "train_data = shuffledSpamDF[holdBackIndex:].copy()\n",
    "test_data = shuffledSpamDF[:holdBackIndex].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.train(train_data, 'is_spam', 'normalized_emails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# triplets (subject, actual is_spam, predicted spam probability)\n",
    "classified = [(row['category'], classifier.classify(row['normalized_emails']))\n",
    "              for index, row in test_data.iterrows()]\n",
    "\n",
    "# assume that spam_probability > 0.5 corresponds to spam prediction # and count the combinations of (actual is_spam, predicted is_spam)\n",
    "counts = collections.Counter((actual == 'spam', predicted_probability > 0.5)\n",
    "                     for actual, predicted_probability in classified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how we did! We will do so by computing the Precision, Recall and F-measure or harmonic mean between Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision = counts[(True,True)]/(counts[(False,True)]+counts[(True,True)]) #True positives over all positive predictions\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall = counts[(True,True)]/(counts[(True,False)]+counts[(True,True)])#what fraction of positives identified\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_measure = 2 * (precision * recall)/(precision + recall)\n",
    "print (f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the emails are classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['predicted_spam_probability'] = [classifier.classify(d) for d in test_data['normalized_emails']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the ROC curve, and integrate to get the area under the curve (AUC), or probability mass associated with correct classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, _ = sklearn.metrics.roc_curve(test_data['is_spam'], test_data['predicted_spam_probability'])\n",
    "roc_auc = sklearn.metrics.auc(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x,y, color = 'darkorange', lw = 2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your thoughts*</span>\n",
    "\n",
    "<span style=\"color:red\">What will happen if I change my spam threshold from 0.5?\n",
    "\n",
    "<span style=\"color:red\">What decision rule are we using, when we assign the class label $\\hat{y} = S$ when $P(S\\,|\\,X_1\\dots X_n) = \\frac{P(S)\\prod_i P(X_i = x_i|S)}{P(\\textbf{X})} > 0.5$\n",
    "\n",
    "<span style=\"color:red\">What will happen if we split the data differently (e.g., less training data, more testing data). \n",
    "\n",
    "<span style=\"color:red\">**Try it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find words that lead to a high probability of spam (using Bayes' Theorem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"uses bayes's theorem to compute p(spam | message contains word)\"\"\"\n",
    "    # word_prob is one of the triplets produced by word_probabilities\n",
    "\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sorted(classifier.word_probs,key=p_spam_given_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spammiest_words = words[-15:]\n",
    "hammiest_words = words[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spammiest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hammiest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with a corpus of Clinton / Obama Press Releases\n",
    "\n",
    "While the Clinton and Obama Senatorial Press are not hand-coded, we can imagine that we have been given a stack of such press releases, but lost the metadata associated with which senatorial office issued which. If we label a few of them, how well can our classifier do at recovering the rest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ObamaClintonReleases = pandas.read_csv(\"data/ObamaClintonReleases.csv\")\n",
    "ObamaClintonReleases = ObamaClintonReleases.dropna(axis=0, how='any')\n",
    "ObamaClintonReleases['tokenized_text'] = ObamaClintonReleases['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "ObamaClintonReleases['normalized_text'] = ObamaClintonReleases['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the 'targetSenator' column into a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ObamaClintonReleases['IsObama'] = [s == 'Obama' for s in ObamaClintonReleases['targetSenator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "ObamaClintonReleases = ObamaClintonReleases.reindex(np.random.permutation(ObamaClintonReleases.index))\n",
    "holdBackFraction = .25\n",
    "holdBackIndex = int(holdBackFraction * len(ObamaClintonReleases))\n",
    "train_data = ObamaClintonReleases[holdBackIndex:].copy()\n",
    "test_data = ObamaClintonReleases[:holdBackIndex].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (len(train_data))\n",
    "print (len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try with a logistic regression, which may be familiar to you from statistical methods classes. First, we must turn the training dataset into a tf-idf matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "TFVects = TFVectorizer.fit_transform(train_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use the CountVectorizer instead, which simply produces a matrix of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TFVects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a regression, we cannot have more variables than cases. So, we need to first do a dimension reduction. First, we will approah this with PCA. You have previously seen this in week 3. Here we are not concerned about visualization but rather classification and so all principal components are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PCA = sklearn.decomposition.PCA\n",
    "pca = PCA().fit(TFVects.toarray())\n",
    "reduced_data = pca.transform(TFVects.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colordict = {\n",
    "True: 'red',\n",
    "False: 'blue',\n",
    "    }\n",
    "colors = [colordict[c] for c in train_data['IsObama']]\n",
    "fig = plt.figure(figsize = (5,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "ax.scatter(reduced_data[:, 0], reduced_data[:, 1], color = colors, alpha = 0.5, label = colors)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('True Classes, Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA cannot distinguish Clinton from Obama very well. Let's perform a screeplot to see how many dimensions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = TFVects.shape[0]\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "eigen_vals = np.arange(n) + 1\n",
    "ax1.plot(eigen_vals, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "ax1.set_title('Scree Plot')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "eigen_vals = np.arange(20) + 1\n",
    "ax2.plot(eigen_vals, pca.explained_variance_ratio_[:20], 'ro-', linewidth=2)\n",
    "ax2.set_title('Scree Plot (First 20 Principal Components)')\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Proportion of Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the first 10 pricipal components as our covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform our predictor variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = np.array([int(label) for label in train_data['IsObama']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a logistic regression to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the logistic regression performs on the training dataset on we develop the model. Unfortunately, mean accuracy is only about 68%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it perform on the testing dataset, which we \"held out\" and did not use for model training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TFVects_test = TFVectorizer.transform(test_data['text'])\n",
    "reduced_data_test = pca.transform(TFVects_test.toarray())\n",
    "X_test = reduced_data_test[:, :10]\n",
    "Y_test = np.array([int(label) for label in test_data['IsObama']])\n",
    "logistic.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly poorer. How about using more dimensions (40)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :40]\n",
    "logistic.fit(X, Y)\n",
    "X_test = reduced_data_test[:, :40]\n",
    "print(logistic.score(X,Y))\n",
    "print(logistic.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or still more (100)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :100]\n",
    "logistic.fit(X, Y)\n",
    "X_test = reduced_data_test[:, :100]\n",
    "print(logistic.score(X,Y))\n",
    "print(logistic.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even more (200)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :200]\n",
    "logistic.fit(X, Y)\n",
    "X_test = reduced_data_test[:, :200]\n",
    "print(logistic.score(X,Y))\n",
    "print(logistic.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is becoming ridiculous (400)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :400]\n",
    "logistic.fit(X, Y)\n",
    "X_test = reduced_data_test[:, :400]\n",
    "print(logistic.score(X,Y))\n",
    "print(logistic.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of covariates would overfit our data, and it seems that using a logistic regression, our prediction accuracy is at best about 80%. We can, however, try a logistic regression that uses L1 regularization or the L1-norm loss function, which is also known as least absolute deviations (LAD), least absolute errors (LAE) or L1 penalty. It minimizes the sum of the absolute differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$):\n",
    "\n",
    "$S=\\sum^n_{i=1}|y_i=f(x_i)|$\n",
    "\n",
    "The result is a model retaining only the most individually significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = TFVects\n",
    "logistic_l1= linear_model.LogisticRegression(penalty='l1')\n",
    "logistic_l1.fit(X, Y)\n",
    "print(logistic_l1.score(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using training data, and then test it on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(logistic_l1.score(TFVects_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84% accuracy seems like the best we can get by using a logistic regression.\n",
    "\n",
    "Now let's try with Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wc = wordCounts(train_data, 'normalized_text')\n",
    "wc = count_words(train_data, 'normalized_text', 'IsObama')\n",
    "wc['veteran']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data, 'IsObama', 'normalized_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the result on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classified = []\n",
    "for index, row in test_data.iterrows():\n",
    "    classified.append((row['normalized_text'], row['IsObama'], classifier.classify(row['normalized_text'])))\n",
    "\n",
    "# assume that spam_probability > 0.5 corresponds to spam prediction # and count the combinations of (actual is_spam, predicted is_spam)\n",
    "counts = collections.Counter((actual, predicted_probability > 0.5)\n",
    "                     for text, actual, predicted_probability in classified)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision = counts[(True,True)]/(counts[(False,True)]+counts[(True,True)]) #True positives over all positive predictions\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall = counts[(True,True)]/(counts[(True,False)]+counts[(True,True)])#what fraction of positives identified\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_measure = 2 * (precision * recall)/(precision + recall)\n",
    "print (f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_classification = pandas.DataFrame(classified, columns = ['press release', 'is Obama', 'posterior probability'])\n",
    "df_classification = df_classification.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how well our posterior distribution looks relative to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.grid(False)\n",
    "ax1.set_frame_on(False)\n",
    "df_classification[df_classification['is Obama'] == True]['posterior probability'].hist(alpha = 0.5, ax = ax1, bins = 10, label = 'Obama', color = 'red')\n",
    "df_classification[df_classification['is Obama'] == False]['posterior probability'].hist(alpha = 0.5, ax = ax1, bins = 10, label = 'Clinton', color = 'blue')\n",
    "ax1.set_xlim((0,1.1))\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('posterior')\n",
    "ax1.set_ylabel('counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification is suprisingly accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_obama_given_word(word_prob):\n",
    "    \"\"\"uses bayes's theorem to compute p(spam | message contains word)\"\"\"\n",
    "    # word_prob is one of the triplets produced by word_probabilities\n",
    "\n",
    "    word, prob_if_obama, prob_if_not_obama = word_prob\n",
    "    return prob_if_obama / (prob_if_obama + prob_if_not_obama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sorted(classifier.word_probs,key=p_obama_given_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Obama_words = words[-15:]\n",
    "Clinton_words = words[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the words that distinguish Obama and Clinton press releases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Obama_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Clinton_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to classify our text into one of *many* classes? The multinomial Naive Bayes generating model assumes that document features (e.g., words) are generated by draws from a multinomial distribution (recall this gives the probability to observe a particular pattern of counts across features). \n",
    "\n",
    "Let's use again the dataset we used in week 3, the 20 newsgroup dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(data_home = 'data') #Free data to play with: documents from a newsgroup corpus.\n",
    "data.target_names #Possible categories, i.e., the newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has a built in breakdown into training and testing sets. We can pick specific categories, and pull the relevant training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics'] #Can change these of course\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, data_home = 'data')\n",
    "test = fetch_20newsgroups(subset='test', categories=categories, data_home = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train.data) #See how many training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test.data) #Ditto for testing -- it's about 60% training, 40% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.data[5]) #Look at an example\n",
    "#print(train.target_names[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract features from the text. We can use built-in feature extraction to do so. We will use a tf-idf vectorizer, which converts the document into a vector of words with tf-idf weights (term-frequency inverse-document frequency). This gives high weight to words that show up a lot in a given document but rarely across documents in the corpus (more distinctive). \n",
    "\n",
    "We also take advantage of a useful feature of Scikit-Learn: we can make pipelines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2'), MultinomialNB()) #This applies the vectorizer, then trains Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train.data,train.target) #Training syntax: feed the fit method the training data and the training targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use a confusion matrix, like we used last week for evaluating human coders relative to one another. Now we are evaluating our classifier relative to human coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the precision, recall, and F-measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) #precision\n",
    "print(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) #recall\n",
    "print(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) #F-1 measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ROC curves.\n",
    "First, we need to binarize the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(test.target)\n",
    "print(lb.classes_)\n",
    "print(lb.transform([1, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = lb.transform(test.target)\n",
    "y_score = model.predict_proba(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 4\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = sklearn.metrics.roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = sklearn.metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lw = 2\n",
    "n_classes = 4\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += sp.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] =  sklearn.metrics.auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue', 'olive']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also give the model a string and use the predict method to see if it can assign it to a category. Using our model to extend to new, uncoded data might be the primary purpose of a social science application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model): #We just define a simple function here\n",
    "    return train.target_names[model.predict([s])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict_category('rockets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it yourself with your own strings!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's add Bernie Sanders to our Obama / Clinton face-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ObamaClintonSandersReleases = pandas.read_csv(\"data/ObamaClintonSandersReleases.csv\")\n",
    "ObamaClintonSandersReleases = ObamaClintonSandersReleases.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = list(zip(ObamaClintonSandersReleases['text'], ObamaClintonSandersReleases['targetSenator']))\n",
    "random.seed(0) #This is important for replicability\n",
    "train_data,test_data = split_data(data,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (len(train_data))\n",
    "print (len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.data = [data[0] for data in train_data]\n",
    "train.target = [data[1] for data in train_data]\n",
    "test.data = [data[0] for data in test_data]\n",
    "test.target = [data[1] for data in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train.data,train.target) #Training syntax: feed the fit method the training data and the training targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=['Obama', 'Clinton', 'Sanders'], yticklabels=['Obama', 'Clinton', 'Sanders'])\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the precision, recall, and F-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) #precision\n",
    "print(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) #recall\n",
    "print(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) #F-1 measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the ROC curves, first we need to binarize the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(test.target)\n",
    "print(lb.classes_)\n",
    "print(lb.transform(['Clinton', 'Obama']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = lb.transform(test.target)\n",
    "y_score = model.predict_proba(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = sklearn.metrics.roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = sklearn.metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += sp.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] =  sklearn.metrics.auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also give the model a string and use the predict method assign it to a category. This extrapolation to new, untagged data will likely be the main point of a social science application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(['money'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(['Wall Street'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(['Chicago'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform Logistic and Naïve Bayes classification (binary or multinomial) using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project (e.g., these could be crowd-sourced codes gathered through Amazon Mechanical Turk last week). Visualize the confusion matrix for training and testing sets. Calculate precision, recall, the F-measure, and AUC, then perform an ROC visualization. How do these classifiers perform? Exrapolate codes from these models to all uncoded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now return to the python Machine learning package, Scikit-Learn, for a decision tree example. Decision trees can be used to predict both categorical/class labels (i.e., classification) and continuous labels (i.e., regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how these work, we first create some artificial data to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import our Decision Tree classifier from sklearn.tree (familiar syntax) and fit it using the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4,random_state=0).fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's going on visually with the classification, we can use this (complex) visualizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, Xmod, ymod, ax=None, cmap='rainbow'): #X and y are plotted; Xmod and ymod train\n",
    "    ax = ax or plt.gca()\n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # fit the estimator\n",
    "    model.fit(Xmod, ymod)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnew, ynew = make_blobs(n_samples=1000, centers=4,\n",
    "                random_state=0, cluster_std=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_classifier(DecisionTreeClassifier(max_depth=4,random_state=0), Xnew, ynew, X, y) #We train on the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(ynew,DecisionTreeClassifier(max_depth=10,random_state=0).fit(X,y).predict(Xnew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depthvec = []\n",
    "scorevec = []\n",
    "for i in range(1,20):\n",
    "    tree2 = DecisionTreeClassifier(max_depth=i,random_state=0).fit(X,y)\n",
    "    score = accuracy_score(ynew,tree2.predict(Xnew))\n",
    "    depthvec.append(i)\n",
    "    scorevec.append(score)\n",
    "plt.scatter(depthvec,scorevec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We an select different layers of the tree or \"prune\" the decision tree. At approximately four layers down the decision tree, the shape is somewhat odd, suggesting that our model is overfitting beyond those four layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.5,random_state=1) #test_size means a 50/50 split\n",
    "print(len(X))\n",
    "print(len(Xtrain))\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some abuse of notation, we can use the visualizer to plot first a classifier trained on the \"training\" half of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_classifier(DecisionTreeClassifier(max_depth=5,random_state=0), X, y, Xtrain, ytrain) #Train with half the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train on the second half of the data--allegedly the test data--to see how different training sets affect the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_classifier(DecisionTreeClassifier(random_state=0), X, y, Xtest, ytest) #Train with other half of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can get a sense of the performance by training on the training data, but PLOTTING the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_classifier(DecisionTreeClassifier(), Xtest, ytest, Xtrain, ytrain) #Trains with train data, plots test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining multiple overfitting estimators turns out to be a key idea in machine learning. This is called **bagging** and is a type of **ensemble** method. The idea is to make many randomized estimators--each can overfit, as decision trees are wont to do--but then to combine them, ultimately producing a better classification. A **random forest** is produced by bagging decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10) #Create an instance of our decision tree classifier.\n",
    "\n",
    "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1) #Each tree uses up to 80% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?BaggingClassifier #Learn more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag.fit(X,y) #Fit the bagged classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnew, ynew = make_blobs(n_samples=10000, centers=4,\n",
    "                random_state=0, cluster_std=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_classifier(bag,Xnew,ynew,X,y) #And visualize\n",
    "#Remember we can give the full data as training data, as bag automatically splits and trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_score(ynew,bag.predict(Xnew),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try this on the Clinton / Obama / Sanders dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TFVects = TFVectorizer.fit_transform(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4,random_state=0).fit(TFVects,train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFVects_test = TFVectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = tree.predict(TFVects_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=['Obama', 'Clinton', 'Sanders'], yticklabels=['Obama', 'Clinton', 'Sanders'])\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision, recall, and F-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) #precision\n",
    "print(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) #recall\n",
    "print(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) #F-1 measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree is not substantially better than Multinomial Naive Bayes. Let's try a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag.fit(TFVects,train.target) #Fit the bagged classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = bag.predict(TFVects_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=['Obama', 'Clinton', 'Sanders'], yticklabels=['Obama', 'Clinton', 'Sanders'])\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) #precision\n",
    "print(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) #recall\n",
    "print(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) #F-1 measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Aside -- Random forest regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a challenging, hypothetical dataset. While we may never see this in content analysis, we may see cyclical phenomena like we sketch here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(200) #200 uniformly distributed random numbers between 0 and 10\n",
    "def model(x, sigma=0.3): \n",
    "    fast_oscillation = np.sin(5 * x) \n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * rng.randn(len(x)) #Create 200 random numbers, normally distributed\n",
    "\n",
    "    return slow_oscillation + fast_oscillation + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt='o'); #Shows one std around point\n",
    "#plt.scatter(x, y); #Plots the actual points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to learn this with something simple; a single decision tree, but one that is a regressor, not a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit regression models\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_3 = DecisionTreeRegressor(max_depth=10)\n",
    "regr_1.fit(x[:,None], y)\n",
    "regr_2.fit(x[:,None], y)\n",
    "regr_3.fit(x[:,None], y)\n",
    "\n",
    "# Predict\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit_1 = regr_1.predict(xfit[:, None]) #xfit[:,None] is a work-around to pass a 1d feature matrix\n",
    "yfit_2 = regr_2.predict(xfit[:, None])\n",
    "yfit_3 = regr_3.predict(xfit[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these two decision trees do... What do you think will happen as we add depth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
    "plt.plot(xfit, yfit_1, '-r'); #depth = 2\n",
    "plt.plot(xfit,yfit_2,'-b'); #depth = 5\n",
    "plt.plot(xfit,yfit_3,'-g'); #depth = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use a **random forest regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(200) \n",
    "forest.fit(x[:, None], y)\n",
    "    \n",
    "yfit = forest.predict(xfit[:, None]) #Predictions of the forest model\n",
    "ytrue = model(xfit, sigma=0) #This is the underlying data, no noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents a beautiful fit of the underlying pattern. A lot of the noise has been washed out. This exemplifies the power of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform decision tree and random forest classification (binary, multinomial or continuous) using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project. As with the last ***your turn***, these could be crowd-sourced codes gathered through Amazon Mechanical Turk last week. Visualize the classification of data points. Calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). Now build an ensemble classifier by bagging trees into a random forest. Visualize the result. How do these classifiers perform? What does ensemble learning do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest neighbors classifier takes a simpler premise than those before: Find the labeled datapoint in question and \"borrow\" its label.\n",
    "\n",
    "Let's use newsgroup data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['soc.religion.christian', 'sci.space', 'comp.graphics'] #Can change these of course\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, data_home = 'data')\n",
    "test = fetch_20newsgroups(subset='test', categories=categories, data_home = 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, min_df=3, stop_words='english', norm='l2')\n",
    "TFVects = TFVectorizer.fit_transform(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TFVects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduced_data = PCA(n_components = 2).fit_transform(TFVects.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colordict = {\n",
    "0: 'red',\n",
    "1: 'orange',\n",
    "2: 'green',\n",
    "    }\n",
    "colors = [colordict[c] for c in train.target]\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "ax.scatter(reduced_data[:, 0], reduced_data[:, 1], color = colors, alpha = 0.5, label = colors)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('True Classes, Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our k-nearest neighbors classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "weights=\"uniform\"\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purpopose, let's first try applying the k-nearest neighbors classifier to the projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(reduced_data, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TFVects_test = TFVectorizer.transform(test.data)\n",
    "reduced_data_test = PCA(n_components = 2).fit_transform(TFVects_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = clf.predict(reduced_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision, recall, and F-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) #precision\n",
    "print(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) #recall\n",
    "print(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) #F-1 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "x_min, x_max = reduced_data[:, 0].min() - 0.2, reduced_data[:, 0].max() + 0.2\n",
    "y_min, y_max = reduced_data[:, 1].min() - 0.2, reduced_data[:, 1].max() + 0.2\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=colors, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "          % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the full tf-idf matrix instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(TFVects, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = clf.predict(TFVects_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small improvement. In this case, PCA reduced the dimensionality of the data without loosing too much informatin.\n",
    "Finally, let's use it again on the Obama Clinton Sanders dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.data = [data[0] for data in train_data]\n",
    "train.target = [data[1] for data in train_data]\n",
    "test.data = [data[0] for data in test_data]\n",
    "test.target = [data[1] for data in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFVects_OCS = TFVectorizer.fit_transform(train.data)\n",
    "TFVects_OCS_test = TFVectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(TFVects_OCS,train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = clf.predict(TFVects_OCS_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=['Obama', 'Clinton', 'Sanders'], yticklabels=['Obama', 'Clinton', 'Sanders'])\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) #precision\n",
    "print(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) #recall\n",
    "print(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) #F-1 measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform and visualize k-nearest neighbor classification using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project. Visualize the classification of data points and calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). Articulate how the *k*-nearest neighbor approach relates to *k*-means clustering explored in week 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVMs\n",
    "\n",
    "Now we will examine Support Vector Machines, an approach that creates the partition that preserves the maximum margin between classes.\n",
    "\n",
    "We will use a few sub forums from reddit--which tend to share text rather than memes--namely `talesfromtechsupport`, `badroommates`, `weeabootales` and `relationships`. The top 100 text posts from each have been saved to `data/reddit.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditDf = pandas.read_csv('data/reddit.csv')\n",
    "\n",
    "#Drop a couple missing values\n",
    "\n",
    "redditDf = redditDf.dropna()\n",
    "\n",
    "#tokenize and normalize\n",
    "redditDf['tokenized_text'] = redditDf['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "redditDf['normalized_text'] = redditDf['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tf.idf the data to make our vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, min_df=3, stop_words='english', norm='l2')\n",
    "redditTFVects = redditTFVectorizer.fit_transform([' '.join(l) for l in redditDf['normalized_text']])\n",
    "redditDf['tfVect'] = [np.array(v) for v in redditTFVects.todense()]\n",
    "redditDf['tfVect'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditDf['tfVect'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = sklearn.svm.SVC(kernel='linear', probability = False)\n",
    "#probability = True is slower but  lets you call predict_proba()\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "shuffled_redditDf = redditDf.reindex(np.random.permutation(redditDf.index))\n",
    "holdBackFraction = .2\n",
    "holdBackIndex = int(holdBackFraction * len(shuffled_redditDf))\n",
    "training_redditDf = shuffled_redditDf[holdBackIndex: ].copy()\n",
    "test_redditDf = shuffled_redditDf[:holdBackIndex].copy()\n",
    "print(len(training_redditDf))\n",
    "print(len(test_redditDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(np.stack(training_redditDf['tfVect'], axis=1)[0], training_redditDf['subreddit'].as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 4 classes of posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a prediction from the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(clf.predict_proba(test_redditDf['tfVect'][82]))\n",
    "print(clf.predict(test_redditDf['tfVect'][82]))\n",
    "print(test_redditDf['subreddit'][82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditDf['svc'] = [clf.predict(v)[0] for v in redditDf['tfVect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(redditDf['svc'], redditDf['subreddit'])\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=redditDf['subreddit'].unique(), yticklabels=redditDf['subreddit'].unique())\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "\n",
    "We include an example of a simple neural network, the Multi-layer Perceptron (MLP) that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. The following figure shows a one hidden layer MLP with scalar output. ![title](data/multilayerperceptron_network.png) The leftmost layer, known as the input layer, consists of a set of \"neurons\" $\\{x_i | x_1, x_2, ..., x_m\\}$ representing the input features (e.g., weighted words). Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation $w_1x_1 + w_2x_2 + ... + w_mx_m$, followed by a non-linear activation function $g(\\cdot):R \\rightarrow R$ - like the logistic or hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(np.stack(training_redditDf['tfVect'], axis=1)[0], training_redditDf['subreddit'].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clf.predict_proba(test_redditDf['tfVect'][82]))\n",
    "print(clf.predict(test_redditDf['tfVect'][82]))\n",
    "print(test_redditDf['subreddit'][82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_redditDf['MLP'] = [clf.predict(v)[0] for v in test_redditDf['tfVect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test_redditDf['MLP'], test_redditDf['subreddit'])\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=redditDf['subreddit'].unique(), yticklabels=redditDf['subreddit'].unique())\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform a neural network classification and calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). How does this classify relevant to *k*-nearest neighbor, Naive Bayes, logistic and decision-tree approaches?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
