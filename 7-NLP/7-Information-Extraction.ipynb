{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here]('http://nlp.stanford.edu:8080/corenlp/') to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.parse import stanford\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw.tree import TreeView\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import graphviz #You also need to install the commandline graphviz\n",
    "import re\n",
    "\n",
    "import IPython.display\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the [Stanford NLP group](http://nlp.stanford.edu/) programs with nltk on your own machine (you do *not* need to do this for this assignment), it will require a little bit of setup. We are basing these instructions on those provided by nltk, [here](https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#stanford-tagger-ner-tokenizer-and-parser), but with small changes to work with our notebooks. We also note that lower performance versions of many of the techniques demonstrated here are available natively within nltk (see the updated [nltk book](http://www.nltk.org/book/)).\n",
    "\n",
    "1. Install [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\n",
    "    + Make sure your `JAVAPATH` is setup if you're on windows\n",
    "2. Download the following zip files from the Stanford NLP group, where DATE is the release date of the files, this will be the value of `stanfordVersion`\n",
    "    + [`stanford-corenlp-full-2016-10-31.zip`](https://stanfordnlp.github.io/CoreNLP/)\n",
    "    + [`stanford-postagger-full-DATE.zip`](http://nlp.stanford.edu/software/tagger.html#Download)\n",
    "    + [`stanford-ner-DATE.zip`](http://nlp.stanford.edu/software/CRF-NER.html#Download)\n",
    "    + [`stanford-parser-full-DATE.zip`](http://nlp.stanford.edu/software/lex-parser.html#Download)\n",
    "3. Unzip the files and place the resulting directories in the same location, this will become `stanfordDir`\n",
    "4. Lookup the version number used by the parser `stanford-parser-VERSION-models.jar` and set to to be `parserVersion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the date at the end of each of the zip files, e.g.\n",
    "#the date in stanford-ner-2016-10-31.zip\n",
    "stanfordVersion = '2016-10-31'\n",
    "\n",
    "#This is the version numbers of the parser models, these\n",
    "#are files in `stanford-parser-full-2016-10-31.zip`, e.g.\n",
    "#stanford-parser-3.7.0-models.jar\n",
    "parserVersion = '3.7.0'\n",
    "\n",
    "#This is where the zip files were unzipped.Make sure to\n",
    "#unzip into directories named after the zip files\n",
    "#Don't just put all the files in `stanford-NLP`\n",
    "stanfordDir = '/mnt/efs/resources/shared/stanford-NLP'\n",
    "\n",
    "#Parser model, there are a few for english and a couple of other languages as well\n",
    "modelName = 'englishPCFG.ser.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will initialize all the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up [NER tagger](http://www.nltk.org/api/nltk.tag.html?highlight=stanfordpostagger#nltk.tag.stanford.StanfordNERTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerClassifierPath = os.path.join(stanfordDir,'stanford-ner-{}'.format(stanfordVersion), 'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "nerJarPath = os.path.join(stanfordDir,'stanford-ner-{}'.format(stanfordVersion), 'stanford-ner.jar')\n",
    "\n",
    "nerTagger = StanfordNERTagger(nerClassifierPath, nerJarPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up [POS Tagger](http://www.nltk.org/api/nltk.tag.html?highlight=stanfordpostagger#nltk.tag.stanford.StanfordPOSTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postClassifierPath = os.path.join(stanfordDir, 'stanford-postagger-full-{}'.format(stanfordVersion), 'models/english-bidirectional-distsim.tagger')\n",
    "\n",
    "postJarPath = os.path.join(stanfordDir,'stanford-postagger-full-{}'.format(stanfordVersion), 'stanford-postagger.jar')\n",
    "\n",
    "postTagger = StanfordPOSTagger(postClassifierPath, postJarPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up [Parser](http://www.nltk.org/api/nltk.parse.html?highlight=stanfordparser#module-nltk.parse.stanford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parserJarPath = os.path.join(stanfordDir, 'stanford-parser-full-{}'.format(stanfordVersion), 'stanford-parser.jar')\n",
    "\n",
    "parserModelsPath = os.path.join(stanfordDir, 'stanford-parser-full-{}'.format(stanfordVersion), 'stanford-parser-{}-models.jar'.format(parserVersion))\n",
    "\n",
    "modelPath = os.path.join(stanfordDir, 'stanford-parser-full-{}'.format(stanfordVersion), modelName)\n",
    "\n",
    "#The model files are stored in the jar, we need to extract them for nltk to use\n",
    "if not os.path.isfile(modelPath):\n",
    "    with zipfile.ZipFile(parserModelsPath) as zf:\n",
    "        with open(modelPath, 'wb') as f:\n",
    "            f.write(zf.read('edu/stanford/nlp/models/lexparser/{}'.format(modelName)))\n",
    "\n",
    "parser = stanford.StanfordParser(parserJarPath, parserModelsPath, modelPath)\n",
    "\n",
    "depParser = stanford.StanfordDependencyParser(parserJarPath, parserModelsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Information Extraction is a module packaged within the Stanford Core NLP package, but it is not yet supported by `nltk`. As a result, we will be defining our own function that runs the Stanford Core NLP java code right here. For other projects, it is often useful to use Java or other programs (in C, C++) within a python workflow, and this is an example. `openIE()` takes in a string or list of strings and then produces as output all the subject, verb, object (SVO) triples Stanford Corenlp can find, as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Watch out, this will very rarely raise an error since it trusts stanford-corenlp \n",
    "def openIE(target):\n",
    "    if isinstance(target, list):\n",
    "        target = '\\n'.join(target)\n",
    "    #setup the java targets\n",
    "    coreDir = '{}/stanford-corenlp-full-{}'.format(stanfordDir, stanfordVersion)\n",
    "    cp = '{0}/stanford-corenlp-{1}.jar:{0}/stanford-corenlp-{1}-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar'.format(coreDir, parserVersion)\n",
    "    with tempfile.NamedTemporaryFile(mode = 'w', delete = False) as f:\n",
    "        #Core nlp requires a files, so we will make a temp one to pass to it\n",
    "        #This file should be deleted by the OS soon after it has been used\n",
    "        f.write(target)\n",
    "        f.seek(0)\n",
    "        print(\"Starting OpenIE run\")\n",
    "        #If you know what these options do then you should mess with them on your own machine and not the shared server\n",
    "        sp = subprocess.run(['java', '-mx2g', '-cp', cp, 'edu.stanford.nlp.naturalli.OpenIE', '-threads', '1', f.name], stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "        #Live stderr is non-trivial so this is the best we can do\n",
    "        print(sp.stderr.decode('utf-8'))\n",
    "        retSting = sp.stdout.decode('utf-8')\n",
    "    #Making the DataFrame, again having to pass a fake file, yay POSIX I guess\n",
    "    with io.StringIO(retSting) as f:\n",
    "        df = pandas.read_csv(f, delimiter = '\\t', names =['certainty', 'subject', 'verb', 'object'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets illustrate these tools on some *very* short examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. The Stanford POS tagger uses the [Penn Treebank tag set]('http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports') to POS tag words from input sentences. As discussed in the second assignment, this is a relatively precise tagset, which allows more informative tags, and also more opportunities to err :-).\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NNS'), ('.', '.')], [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')], [('While', 'IN'), ('in', 'IN'), ('France', 'NNP'), (',', ','), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('interview', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Wall', 'NNP'), ('Street', 'NNP'), ('Journal', 'NNP'), ('.', '.')], [('Trayvon', 'NNP'), ('Benjamin', 'NNP'), ('Martin', 'NNP'), ('was', 'VBD'), ('an', 'DT'), ('African', 'NNP'), ('American', 'NNP'), ('from', 'IN'), ('Miami', 'NNP'), ('Gardens', 'NNP'), (',', ','), ('Florida', 'NNP'), (',', ','), ('who', 'WP'), (',', ','), ('at', 'IN'), ('17', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('was', 'VBD'), ('fatally', 'RB'), ('shot', 'VBN'), ('by', 'IN'), ('George', 'NNP'), ('Zimmerman', 'NNP'), (',', ','), ('a', 'DT'), ('neighborhood', 'NN'), ('watch', 'NN'), ('volunteer', 'NN'), (',', ','), ('in', 'IN'), ('Sanford', 'NNP'), (',', ','), ('Florida', 'NNP'), ('.', '.')], [('Buffalo', 'NNP'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "pos_sents = postTagger.tag_sents(tokenized_text)\n",
    "print(pos_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite good. Now we will try POS tagging with a somewhat larger corpus. We consider a few of the top posts from the reddit data we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('data/reddit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the 10 highest scoring posts and tokenizing the sentences. Once again, notice that we aren't going to do any kind of stemming this week (although *semantic* normalization is sometimes performed where we translate synonyms into the same focal word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>brenkelieshere</td>\n",
       "      <td>False</td>\n",
       "      <td>9448</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>Last year, Help Desk got a call from a user co...</td>\n",
       "      <td>How to fix a laptop that won't boot in under a...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[Last, year, ,, Help, Desk, got, a, call, fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>Bombadils</td>\n",
       "      <td>False</td>\n",
       "      <td>10528</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>First post in quite some time! I work at a loc...</td>\n",
       "      <td>OK, now the password is 'D35p41r'</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[First, post, in, quite, some, time, !], [I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1200</td>\n",
       "      <td>whenlifegivesyoushit</td>\n",
       "      <td>False</td>\n",
       "      <td>11003</td>\n",
       "      <td>Relationships</td>\n",
       "      <td>[Original Post](https://www.reddit.com/r/relat...</td>\n",
       "      <td>[UPDATE]My [26 F] with my husband [29 M] 1 yea...</td>\n",
       "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
       "      <td>[[[, Original, Post, ], (, https, :, //www.red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>sfsdfd</td>\n",
       "      <td>False</td>\n",
       "      <td>11295</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>I witnessed this astounding IT meltdown around...</td>\n",
       "      <td>Company-wide email + 30,000 employees + auto-r...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[I, witnessed, this, astounding, IT, meltdown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Raitaro</td>\n",
       "      <td>False</td>\n",
       "      <td>12372</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>I work Helpdesk for a retail store chain in th...</td>\n",
       "      <td>I'm pretty sure I knocked a user out from near...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[I, work, Helpdesk, for, a, retail, store, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>goldie-gold</td>\n",
       "      <td>False</td>\n",
       "      <td>12650</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>This just happened...  So, I had a laptop syst...</td>\n",
       "      <td>Engineer is doing drugs!! No. No they aren't.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[This, just, happened, ...], [So, ,, I, had, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TheDroolinFool</td>\n",
       "      <td>False</td>\n",
       "      <td>13152</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>Another tale from the out of hours IT desk... ...</td>\n",
       "      <td>\"I need you to fix Google Bing immediately!\"</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[Another, tale, from, the, out, of, hours, IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Clickity_clickity</td>\n",
       "      <td>False</td>\n",
       "      <td>13404</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>[Part 1](http://www.reddit.com/r/talesfromtech...</td>\n",
       "      <td>Jack, the Worst End User, Part 4</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[[, Part, 1, ], (, http, :, //www.reddit.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SECGaz</td>\n",
       "      <td>False</td>\n",
       "      <td>13724</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>&gt; $Me  - Hello, IT.   &gt; $Usr - Hi, I am still ...</td>\n",
       "      <td>Hi, I am still off sick but I am not.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[&gt;, $, Me, -, Hello, ,, IT, .], [&gt;, $, Usr, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>guitarsdontdance</td>\n",
       "      <td>False</td>\n",
       "      <td>14089</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>So my story starts on what was a normal day ta...</td>\n",
       "      <td>\"Don't bother sending a tech, I'll be dead by ...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[So, my, story, starts, on, what, was, a, nor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                author over_18  score                subreddit  \\\n",
       "9           8        brenkelieshere   False   9448  Tales From Tech Support   \n",
       "8           7             Bombadils   False  10528  Tales From Tech Support   \n",
       "7        1200  whenlifegivesyoushit   False  11003            Relationships   \n",
       "6           6                sfsdfd   False  11295  Tales From Tech Support   \n",
       "5           5               Raitaro   False  12372  Tales From Tech Support   \n",
       "4           4           goldie-gold   False  12650  Tales From Tech Support   \n",
       "3           3        TheDroolinFool   False  13152  Tales From Tech Support   \n",
       "2           2     Clickity_clickity   False  13404  Tales From Tech Support   \n",
       "1           1                SECGaz   False  13724  Tales From Tech Support   \n",
       "0           0      guitarsdontdance   False  14089  Tales From Tech Support   \n",
       "\n",
       "                                                text  \\\n",
       "9  Last year, Help Desk got a call from a user co...   \n",
       "8  First post in quite some time! I work at a loc...   \n",
       "7  [Original Post](https://www.reddit.com/r/relat...   \n",
       "6  I witnessed this astounding IT meltdown around...   \n",
       "5  I work Helpdesk for a retail store chain in th...   \n",
       "4  This just happened...  So, I had a laptop syst...   \n",
       "3  Another tale from the out of hours IT desk... ...   \n",
       "2  [Part 1](http://www.reddit.com/r/talesfromtech...   \n",
       "1  > $Me  - Hello, IT.   > $Usr - Hi, I am still ...   \n",
       "0  So my story starts on what was a normal day ta...   \n",
       "\n",
       "                                               title  \\\n",
       "9  How to fix a laptop that won't boot in under a...   \n",
       "8                  OK, now the password is 'D35p41r'   \n",
       "7  [UPDATE]My [26 F] with my husband [29 M] 1 yea...   \n",
       "6  Company-wide email + 30,000 employees + auto-r...   \n",
       "5  I'm pretty sure I knocked a user out from near...   \n",
       "4      Engineer is doing drugs!! No. No they aren't.   \n",
       "3       \"I need you to fix Google Bing immediately!\"   \n",
       "2                   Jack, the Worst End User, Part 4   \n",
       "1              Hi, I am still off sick but I am not.   \n",
       "0  \"Don't bother sending a tech, I'll be dead by ...   \n",
       "\n",
       "                                                 url  \\\n",
       "9  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "8  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "7  https://www.reddit.com/r/relationships/comment...   \n",
       "6  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "5  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "4  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "0  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "                                           sentences  \n",
       "9  [[Last, year, ,, Help, Desk, got, a, call, fro...  \n",
       "8  [[First, post, in, quite, some, time, !], [I, ...  \n",
       "7  [[[, Original, Post, ], (, https, :, //www.red...  \n",
       "6  [[I, witnessed, this, astounding, IT, meltdown...  \n",
       "5  [[I, work, Helpdesk, for, a, retail, store, ch...  \n",
       "4  [[This, just, happened, ...], [So, ,, I, had, ...  \n",
       "3  [[Another, tale, from, the, out, of, hours, IT...  \n",
       "2  [[[, Part, 1, ], (, http, :, //www.reddit.com/...  \n",
       "1  [[>, $, Me, -, Hello, ,, IT, .], [>, $, Usr, -...  \n",
       "0  [[So, my, story, starts, on, what, was, a, nor...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-10:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, JJ), (year, NN), (,, ,), (Help, NN), ...\n",
       "8    [[(First, JJ), (post, NN), (in, IN), (quite, R...\n",
       "7    [[([, NNP), (Original, NNP), (Post, NNP), (], ...\n",
       "6    [[(I, PRP), (witnessed, VBD), (this, DT), (ast...\n",
       "5    [[(I, PRP), (work, VBP), (Helpdesk, NNP), (for...\n",
       "4    [[(This, DT), (just, RB), (happened, VBN), (.....\n",
       "3    [[(Another, DT), (tale, NN), (from, IN), (the,...\n",
       "2    [[([, NNP), (Part, NNP), (1, CD), (], FW), ((,...\n",
       "1    [[(>, JJR), ($, $), (Me, PRP), (-, :), (Hello,...\n",
       "0    [[(So, RB), (my, PRP$), (story, NN), (starts, ...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of `NN` (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('password', 21),\n",
       " ('(', 19),\n",
       " (')', 14),\n",
       " ('time', 14),\n",
       " ('computer', 12),\n",
       " ('lot', 12),\n",
       " ('email', 11),\n",
       " ('life', 11),\n",
       " ('**Genius**', 10),\n",
       " ('day', 9),\n",
       " ('**Me**', 9),\n",
       " ('system', 9),\n",
       " ('message', 9),\n",
       " ('laptop', 8),\n",
       " ('today', 8),\n",
       " ('story', 8),\n",
       " ('call', 8),\n",
       " ('office', 8),\n",
       " ('part', 8),\n",
       " ('problem', 7)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of top verbs (`VB`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 18),\n",
       " ('have', 17),\n",
       " ('get', 14),\n",
       " ('do', 11),\n",
       " ('change', 9),\n",
       " ('make', 8),\n",
       " ('know', 7),\n",
       " ('say', 7),\n",
       " ('tell', 6),\n",
       " ('look', 6),\n",
       " ('help', 6),\n",
       " ('send', 6),\n",
       " ('go', 5),\n",
       " ('open', 4),\n",
       " ('receive', 4),\n",
       " ('take', 4),\n",
       " ('want', 4),\n",
       " ('use', 4),\n",
       " ('see', 4),\n",
       " ('call', 4)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Visualize the distribution for at least three different parts of speech. What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is also a classification task, which identifies named objects. Included with Stanford NER are a 4 class model trained on the CoNLL 2003 eng.train, a 7 class model trained on the MUC 6 and MUC 7 training data sets, and a 3 class model trained on both data sets plus some additional data (including ACE 2002 and limited data in-house) on the intersection of those class sets. \n",
    "\n",
    "**3 class**:\tLocation, Person, Organization\n",
    "\n",
    "**4 class**:\tLocation, Person, Organization, Misc\n",
    "\n",
    "**7 class**:\tLocation, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "These models each use distributional similarity features, which provide some performance gain at the cost of increasing their size and runtime. Also available are the same models missing those features.\n",
    "\n",
    "(We note that the training data for the 3 class model does not include any material from the CoNLL eng.testa or eng.testb data sets, nor any of the MUC 6 or 7 test or devtest datasets, nor Alan Ritter's Twitter NER data, so all of these would be valid tests of its performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tag our first set of exemplary sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'O'), ('saw', 'O'), ('the', 'O'), ('elephant', 'O'), ('in', 'O'), ('my', 'O'), ('pajamas', 'O'), ('.', 'O')], [('The', 'O'), ('quick', 'O'), ('brown', 'O'), ('fox', 'O'), ('jumped', 'O'), ('over', 'O'), ('the', 'O'), ('lazy', 'O'), ('dog', 'O'), ('.', 'O')], [('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')], [('Trayvon', 'PERSON'), ('Benjamin', 'PERSON'), ('Martin', 'PERSON'), ('was', 'O'), ('an', 'O'), ('African', 'O'), ('American', 'O'), ('from', 'O'), ('Miami', 'LOCATION'), ('Gardens', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), (',', 'O'), ('who', 'O'), (',', 'O'), ('at', 'O'), ('17', 'O'), ('years', 'O'), ('old', 'O'), (',', 'O'), ('was', 'O'), ('fatally', 'O'), ('shot', 'O'), ('by', 'O'), ('George', 'PERSON'), ('Zimmerman', 'PERSON'), (',', 'O'), ('a', 'O'), ('neighborhood', 'O'), ('watch', 'O'), ('volunteer', 'O'), (',', 'O'), ('in', 'O'), ('Sanford', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), ('.', 'O')], [('Buffalo', 'LOCATION'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O'), ('buffalo', 'O'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "classified_sents = nerTagger.tag_sents(tokenized_text)\n",
    "print(classified_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run NER over our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-94a9d3e17dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mredditTopScores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classified_sents'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredditTopScores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnerTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2294\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:66124)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-94a9d3e17dbf>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mredditTopScores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classified_sents'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredditTopScores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnerTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[0;32m---> 94\u001b[0;31m                                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1713\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, have much smaller counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of the corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for that classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Here we will introduce the Stanford Parser by feeding it tokenized text from our initial example sentences. While the parser is a dependency parser, this initial program outputs a simple, self-explanatory phrase-structure representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Trayvon']), Tree('NNP', ['Benjamin']), Tree('NNP', ['Martin'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('DT', ['an']), Tree('NNP', ['African']), Tree('NNP', ['American'])]), Tree('PP', [Tree('IN', ['from']), Tree('NP', [Tree('NP', [Tree('NNP', ['Miami']), Tree('NNPS', ['Gardens'])]), Tree(',', [',']), Tree('NP', [Tree('NNP', ['Florida'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree(',', [',']), Tree('PP', [Tree('IN', ['at']), Tree('ADJP', [Tree('NP', [Tree('CD', ['17']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])])]), Tree(',', [',']), Tree('VP', [Tree('VBD', ['was']), Tree('ADVP', [Tree('RB', ['fatally'])]), Tree('VP', [Tree('VBN', ['shot']), Tree('PP', [Tree('IN', ['by']), Tree('NP', [Tree('NP', [Tree('NNP', ['George']), Tree('NNP', ['Zimmerman'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['neighborhood']), Tree('NN', ['watch']), Tree('NN', ['volunteer'])]), Tree(',', [','])])]), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('NNP', ['Sanford']), Tree(',', [',']), Tree('NNP', ['Florida'])])])])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parses = list(parser.parse_sents(tokenized_text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "fourthSentParseTree = list(parses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(fourthSentParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are a common data structure and there are a large number of things to do with them. What we are intetered in though is the realtionship between different types of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   'an African American from Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')],\n",
       " [('NP',\n",
       "   'Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'NP', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Florida occurs twice\n",
    "\n",
    "We can use the function on any number of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NNP', 'Florida')], [('NNP', 'Florida')]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'NNP', 'Florida')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to to look at the whole tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   ROOT                                                                                                                       \n",
      "                                                                                                                    |                                                                                                                          \n",
      "                                                                                                                    S                                                                                                                         \n",
      "            ________________________________________________________________________________________________________|_______________________________________________________________________________________________________________________   \n",
      "           |                       VP                                                                                                                                                                                                       | \n",
      "           |              _________|______________                                                                                                                                                                                          |  \n",
      "           |             |                        NP                                                                                                                                                                                        | \n",
      "           |             |          ______________|________________                                                                                                                                                                         |  \n",
      "           |             |         |                               PP                                                                                                                                                                       | \n",
      "           |             |         |               ________________|___________                                                                                                                                                             |  \n",
      "           |             |         |              |                            NP                                                                                                                                                           | \n",
      "           |             |         |              |           _________________|____________________________________                                                                                                                        |  \n",
      "           |             |         |              |          |           |     |     |                             SBAR                                                                                                                     | \n",
      "           |             |         |              |          |           |     |     |    __________________________|______________________________                                                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |                                                         S                                                                                        | \n",
      "           |             |         |              |          |           |     |     |   |     ____________________________________________________|_______________________                                                                 |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |                                                 VP                                                               | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |    _____________________________________________|_______                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |                                               VP                                                       | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |      _________________________________________|________________________________________                |  \n",
      "           |             |         |              |          |           |     |     |   |    |           PP             |   |     |     |                      PP                                                          |               | \n",
      "           |             |         |              |          |           |     |     |   |    |    _______|____          |   |     |     |     _________________|__________                                                 |               |  \n",
      "           |             |         |              |          |           |     |     |   |    |   |           ADJP       |   |     |     |    |                            NP                                               PP              | \n",
      "           |             |         |              |          |           |     |     |   |    |   |        ____|____     |   |     |     |    |           _________________|________________________________     ___________|___            |  \n",
      "           NP            |         NP             |          NP          |     NP    |  WHNP  |   |       NP        |    |   |    ADVP   |    |          NP            |           NP                       |   |               NP          | \n",
      "    _______|_______      |    _____|_______       |      ____|_____      |     |     |   |    |   |    ___|____     |    |   |     |     |    |     _____|______       |    _______|_________________       |   |      _________|_____      |  \n",
      "  NNP     NNP     NNP   VBD  DT   NNP     NNP     IN   NNP        NNPS   ,    NNP    ,   WP   ,   IN  CD      NNS   JJ   ,  VBD    RB   VBN   IN  NNP          NNP     ,   DT      NN        NN      NN     ,   IN   NNP        ,    NNP    . \n",
      "   |       |       |     |   |     |       |      |     |          |     |     |     |   |    |   |   |        |    |    |   |     |     |    |    |            |      |   |       |         |       |      |   |     |         |     |     |  \n",
      "Trayvon Benjamin Martin was  an African American from Miami     Gardens  ,  Florida  ,  who   ,   at  17     years old   ,  was fatally shot  by George     Zimmerman  ,   a  neighborhood watch volunteer  ,   in Sanford      ,  Florida  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-d92c237163c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "list(parses[1])[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dependency parsing and graph representations\n",
    "\n",
    "Dependency parsing was developed to robustly capture linguistic dependencies from text. The complex tags associated with these parses are detailed [here]('http://universaldependencies.org/u/overview/syntax.html'). When parsing with the dependency parser, we will work directly from the untokenized text. Note that no *processing* takes place before parsing sentences--we do not remove so-called stop words or anything that plays a syntactic role in the sentence, although anaphora resolution and related normalization may be performed before or after parsing to enhance the value of information extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7fee6cf4fd90>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [5]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'The'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'quick'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'brown'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [2, 3],\n",
      "                                      'det': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'fox'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'nmod': [9],\n",
      "                                      'nsubj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'jumped'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'over'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'lazy'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [8],\n",
      "                                      'case': [6],\n",
      "                                      'det': [7]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nmod',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'dog'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "secondSentDepParseTree = list(depParses[1])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph and we can convert it to a dot file and use that to visulize it. Try traversing the tree and extracting elements that are nearby one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"467pt\" height=\"305pt\"\n",
       " viewBox=\"0.00 0.00 467.00 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-301 463,-301 463,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (jumped)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M250.5,-260.799C250.5,-249.163 250.5,-233.548 250.5,-220.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254,-220.175 250.5,-210.175 247,-220.175 254,-220.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"127.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M225.609,-173.799C206.937,-160.895 181.185,-143.099 160.75,-128.978\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.57,-125.981 152.354,-123.175 158.591,-131.74 162.57,-125.981\"/>\n",
       "<text text-anchor=\"middle\" x=\"211.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node7\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (dog)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M263.856,-173.799C273.339,-161.587 286.224,-144.992 296.873,-131.278\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"299.795,-133.221 303.164,-123.175 294.266,-128.927 299.795,-133.221\"/>\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"108.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (quick)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\"><title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"195.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (brown)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.466,-86.799C92.7052,-74.1257 72.4484,-56.7335 56.1474,-42.7377\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"58.3715,-40.0342 48.5043,-36.1754 53.8115,-45.3452 58.3715,-40.0342\"/>\n",
       "<text text-anchor=\"middle\" x=\"93\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.655,-86.799C121.054,-75.1626 117.564,-59.5479 114.588,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"117.936,-45.1711 112.339,-36.1754 111.105,-46.6981 117.936,-45.1711\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.261,-86.799C151.031,-74.5865 164.306,-57.9921 175.278,-44.2776\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178.246,-46.1706 181.76,-36.1754 172.78,-41.7977 178.246,-46.1706\"/>\n",
       "<text text-anchor=\"middle\" x=\"182\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"279.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (over)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>9&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M309.012,-86.799C303.847,-74.9322 296.88,-58.9279 291.013,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.177,-43.9475 286.976,-36.1754 287.758,-46.7414 294.177,-43.9475\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"354.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (the)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M324.19,-86.799C329.495,-74.9322 336.65,-58.9279 342.676,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.935,-46.7332 346.822,-36.1754 339.545,-43.8762 345.935,-46.7332\"/>\n",
       "<text text-anchor=\"middle\" x=\"347\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"429.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (lazy)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M339.367,-86.799C356.368,-74.0105 379.758,-56.4169 398.449,-42.3569\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"400.779,-44.9838 406.667,-36.1754 396.571,-39.3898 400.779,-44.9838\"/>\n",
       "<text text-anchor=\"middle\" x=\"396\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fee6d4b3be0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondSentGraph = graphviz.Source(secondSentDepParseTree.to_dot())\n",
    "secondSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"916pt\" height=\"566pt\"\n",
       " viewBox=\"0.00 0.00 916.00 566.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 562)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-562 912,-562 912,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M232,-521.799C232,-510.163 232,-494.548 232,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.5,-481.175 232,-471.175 228.5,-481.175 235.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\"><title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"74\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.462,-434.963C181.635,-429.483 168.668,-423.214 157,-417 141.139,-408.552 124.062,-398.448 109.546,-389.541\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.218,-386.459 100.872,-384.175 107.535,-392.412 111.218,-386.459\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"158\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.025,-434.799C206.293,-422.471 191.673,-405.679 179.669,-391.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.159,-389.42 172.953,-384.175 176.879,-394.016 182.159,-389.42\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M232,-434.799C232,-423.163 232,-407.548 232,-394.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.5,-394.175 232,-384.175 228.5,-394.175 235.5,-394.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"316\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.999,-434.799C261.295,-422.356 278.087,-405.364 291.784,-391.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.487,-393.748 299.027,-384.175 289.508,-388.828 294.487,-393.748\"/>\n",
       "<text text-anchor=\"middle\" x=\"309\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"417\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.249,-440.698C297.554,-434.849 321.499,-426.853 342,-417 357.504,-409.548 373.521,-399.277 386.753,-389.982\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"388.929,-392.729 395.012,-384.054 384.847,-387.042 388.929,-392.729\"/>\n",
       "<text text-anchor=\"middle\" x=\"384\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"145\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.322,-347.799C62.7596,-336.047 56.6219,-320.238 51.4211,-306.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54.5501,-305.231 47.6681,-297.175 48.0246,-307.764 54.5501,-305.231\"/>\n",
       "<text text-anchor=\"middle\" x=\"89\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.456,-347.911C109.458,-342.777 116.513,-336.704 122,-330 127.594,-323.167 132.146,-314.75 135.68,-306.785\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.96,-308.01 139.487,-297.429 132.476,-305.372 138.96,-308.01\"/>\n",
       "<text text-anchor=\"middle\" x=\"161\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"278\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.842,-348.02C361.179,-342.848 348.831,-336.729 338,-330 326.003,-322.547 313.837,-312.882 303.624,-304.039\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"305.696,-301.199 295.893,-297.181 301.051,-306.435 305.696,-301.199\"/>\n",
       "<text text-anchor=\"middle\" x=\"350\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"365\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M387.324,-347.997C380.976,-342.998 374.988,-336.967 371,-330 367.118,-323.219 365.215,-315.078 364.383,-307.354\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"367.871,-307.038 363.826,-297.246 360.882,-307.423 367.871,-307.038\"/>\n",
       "<text text-anchor=\"middle\" x=\"400\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node12\" class=\"node\"><title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M424.285,-347.799C429.311,-335.932 436.089,-319.928 441.798,-306.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"445.049,-307.749 445.726,-297.175 438.603,-305.019 445.049,-307.749\"/>\n",
       "<text text-anchor=\"middle\" x=\"460\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"545\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M454.782,-347.832C465.178,-342.578 476.265,-336.465 486,-330 497.561,-322.322 509.406,-312.693 519.419,-303.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"521.885,-306.43 527.017,-297.162 517.225,-301.207 521.885,-306.43\"/>\n",
       "<text text-anchor=\"middle\" x=\"521\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">appos</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\"><title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"247\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M420.212,-263.981C417.126,-262.889 414.023,-261.874 411,-261 367.617,-248.462 352.222,-261.441 311,-243 296.707,-236.606 282.772,-226.351 271.573,-216.788\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"273.739,-214.031 263.937,-210.012 269.093,-219.267 273.739,-214.031\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\"><title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"328\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M420.439,-262.889C409.204,-257.185 396.749,-250.298 386,-243 374.63,-235.281 362.986,-225.644 353.143,-216.896\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"355.434,-214.248 345.675,-210.124 350.732,-219.434 355.434,-214.248\"/>\n",
       "<text text-anchor=\"middle\" x=\"401\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\"><title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"408\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M438.132,-260.935C433.834,-255.453 429.393,-249.192 426,-243 422.054,-235.799 418.676,-227.562 415.948,-219.879\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"419.218,-218.62 412.744,-210.233 412.575,-220.826 419.218,-218.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"447.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\"><title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"497\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M461.904,-260.799C468.107,-248.817 476.492,-232.617 483.512,-219.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"486.62,-220.665 488.109,-210.175 480.404,-217.447 486.62,-220.665\"/>\n",
       "<text text-anchor=\"middle\" x=\"500.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\"><title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"610\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.77,-263.362C498.753,-257.34 513.722,-250.116 527,-243 542.84,-234.511 559.913,-224.4 574.432,-215.499\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"576.441,-218.372 583.108,-210.138 572.761,-212.417 576.441,-218.372\"/>\n",
       "<text text-anchor=\"middle\" x=\"568\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\"><title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"777\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.801,-264.024C488.883,-262.923 491.983,-261.893 495,-261 535.368,-249.046 546.96,-252.389 588,-243 635.077,-232.23 688.208,-217.986 726.121,-207.457\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"727.213,-210.786 735.905,-204.728 725.332,-204.043 727.213,-210.786\"/>\n",
       "<text text-anchor=\"middle\" x=\"661\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\"><title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"295\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\"><title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"376\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\"><title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"376\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376,-86.799C376,-75.1626 376,-59.5479 376,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.5,-46.1754 376,-36.1754 372.5,-46.1755 379.5,-46.1754\"/>\n",
       "<text text-anchor=\"middle\" x=\"401\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M314.903,-173.931C311.224,-168.449 307.539,-162.189 305,-156 302.043,-148.793 299.937,-140.633 298.447,-133.026\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"301.893,-132.414 296.774,-123.138 294.991,-133.582 301.893,-132.414\"/>\n",
       "<text text-anchor=\"middle\" x=\"317\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.714,-173.799C344.48,-161.817 353.628,-145.617 361.286,-132.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"364.431,-133.604 366.301,-123.175 358.336,-130.162 364.431,-133.604\"/>\n",
       "<text text-anchor=\"middle\" x=\"392\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\"><title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"490\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M584.2,-173.942C575.795,-168.303 566.454,-161.95 558,-156 545.753,-147.382 532.448,-137.676 520.881,-129.125\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"522.922,-126.281 512.805,-123.134 518.751,-131.903 522.922,-126.281\"/>\n",
       "<text text-anchor=\"middle\" x=\"570\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\"><title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"578\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M600.318,-173.874C597.397,-168.286 594.347,-161.979 592,-156 589.147,-148.731 586.622,-140.656 584.529,-133.154\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"587.892,-132.18 581.954,-123.406 581.125,-133.969 587.892,-132.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"621\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\"><title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"685\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge24\" class=\"edge\"><title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M631.648,-173.733C637.863,-168.342 644.453,-162.181 650,-156 656.652,-148.587 663.138,-139.876 668.683,-131.821\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"671.645,-133.688 674.294,-123.429 665.826,-129.797 671.645,-133.688\"/>\n",
       "<text text-anchor=\"middle\" x=\"677\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\"><title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"777\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\"><title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M777,-173.799C777,-162.163 777,-146.548 777,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"780.5,-133.175 777,-123.175 773.5,-133.175 780.5,-133.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"789\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\"><title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"865\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\"><title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M794.808,-173.799C807.69,-161.356 825.282,-144.364 839.631,-130.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"842.457,-132.64 847.218,-123.175 837.594,-127.605 842.457,-132.64\"/>\n",
       "<text text-anchor=\"middle\" x=\"855\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\"><title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"581\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge27\" class=\"edge\"><title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M663.954,-86.799C648.448,-74.1257 627.168,-56.7335 610.044,-42.7377\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"611.972,-39.7938 602.015,-36.1754 607.543,-45.2138 611.972,-39.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\"><title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"685\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge25\" class=\"edge\"><title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M685,-86.799C685,-75.1626 685,-59.5479 685,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"688.5,-46.1754 685,-36.1754 681.5,-46.1755 688.5,-46.1754\"/>\n",
       "<text text-anchor=\"middle\" x=\"714\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\"><title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"800\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge26\" class=\"edge\"><title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M718.745,-86.9012C728.181,-81.6054 738.244,-75.453 747,-69 757.239,-61.4544 767.611,-52.1229 776.432,-43.5725\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"779.105,-45.8504 783.742,-36.324 774.177,-40.8797 779.105,-45.8504\"/>\n",
       "<text text-anchor=\"middle\" x=\"793\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fee6ce6a320>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphviz.Source(list(depParses[3])[0].to_dot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can traverse this tree by indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list_iterator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-e3a230697433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdepParses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list_iterator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "HELP...how can we iterate through the parse tree. I would like to ask, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can also do a dependency parse on the reddit sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topPostDepParse = list(depParser.parse_sents(redditTopScores['sentences'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a few seconds, but now lets look at the parse tree from one of the processed sentences.\n",
    "\n",
    "The sentence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So anyway , I get a call from an older gentleman who 's quite bitter and mean right off the bat ( does n't like that I asked for his address / telephone number to verify the account , hates that he has to speak with a machine before reaching an agent , etc . ) .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 7\n",
    "print(' '.join(redditTopScores['sentences'][0][targetSentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leads to a very rich dependancy tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1229pt\" height=\"827pt\"\n",
       " viewBox=\"0.00 0.00 1229.00 827.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 823)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-823 1225,-823 1225,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"195\" y=\"-797.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"195\" y=\"-710.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (get)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195,-782.799C195,-771.163 195,-755.548 195,-742.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.5,-742.175 195,-732.175 191.5,-742.175 198.5,-742.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-753.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (So)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.995,-707.618C144.938,-702.29 111.414,-692.766 85,-678 72.7302,-671.141 60.7017,-661.418 50.8082,-652.367\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.0351,-649.656 43.3678,-645.321 48.2216,-654.738 53.0351,-649.656\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"111\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (anyway)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.754,-696.008C160.287,-690.714 152.474,-684.536 146,-678 138.881,-670.813 132.204,-662.033 126.63,-653.847\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129.446,-651.758 121.045,-645.302 123.586,-655.587 129.446,-651.758\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"195\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (I)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195,-695.799C195,-684.163 195,-668.548 195,-655.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.5,-655.175 195,-645.175 191.5,-655.175 198.5,-655.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (call)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212.33,-695.905C217.796,-690.318 223.769,-684.004 229,-678 235.87,-670.114 243.012,-661.196 249.278,-653.097\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"252.15,-655.102 255.442,-645.031 246.589,-650.851 252.15,-655.102\"/>\n",
       "<text text-anchor=\"middle\" x=\"254.5\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node7\" class=\"node\"><title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"615\" y=\"-623.3\" font-family=\"Times,serif\" font-size=\"14.00\">34 (number)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;34 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>5&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.234,-707.488C290.934,-693.585 472.218,-656.896 562.445,-638.636\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"563.353,-642.023 572.46,-636.609 561.964,-635.163 563.353,-642.023\"/>\n",
       "<text text-anchor=\"middle\" x=\"441\" y=\"-666.8\" font-family=\"Times,serif\" font-size=\"14.00\">dep</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (a)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.775,-608.799C234.74,-596.241 215.526,-579.049 199.958,-565.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"201.982,-562.235 192.196,-558.175 197.315,-567.452 201.982,-562.235\"/>\n",
       "<text text-anchor=\"middle\" x=\"235.5\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node9\" class=\"node\"><title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">11 (gentleman)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;11 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268,-608.799C268,-597.163 268,-581.548 268,-568.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271.5,-568.175 268,-558.175 264.5,-568.175 271.5,-568.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"284\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node25\" class=\"node\"><title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"566\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">25 (like)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;25 -->\n",
       "<g id=\"edge31\" class=\"edge\"><title>34&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M605.084,-608.799C598.177,-596.817 588.838,-580.617 581.021,-567.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"583.928,-565.091 575.901,-558.175 577.863,-568.587 583.928,-565.091\"/>\n",
       "<text text-anchor=\"middle\" x=\"604\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">dep</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node32\" class=\"node\"><title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"663\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">33 (telephone)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;33 -->\n",
       "<g id=\"edge30\" class=\"edge\"><title>34&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M624.714,-608.799C631.48,-596.817 640.628,-580.617 648.286,-567.057\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"651.431,-568.604 653.301,-558.175 645.336,-565.162 651.431,-568.604\"/>\n",
       "<text text-anchor=\"middle\" x=\"671\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node33\" class=\"node\"><title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"782\" y=\"-536.3\" font-family=\"Times,serif\" font-size=\"14.00\">36 (verify)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;36 -->\n",
       "<g id=\"edge32\" class=\"edge\"><title>34&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M657.848,-611.093C672.736,-605.407 689.363,-598.479 704,-591 719.51,-583.075 735.854,-572.909 749.552,-563.816\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"751.8,-566.521 758.136,-558.029 747.887,-560.717 751.8,-566.521\"/>\n",
       "<text text-anchor=\"middle\" x=\"736\" y=\"-579.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"110\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>11&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.934,-521.975C219.325,-516.444 206.537,-510.145 195,-504 178.857,-495.402 161.385,-485.273 146.505,-476.388\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"147.984,-473.193 137.61,-471.041 144.377,-479.193 147.984,-473.193\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"187\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (an)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.608,-521.799C239.751,-509.356 223.559,-492.364 210.351,-478.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"212.8,-476 203.367,-471.175 207.732,-480.829 212.8,-476\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (older)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268,-521.799C268,-510.163 268,-494.548 268,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271.5,-481.175 268,-471.175 264.5,-481.175 271.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\"><title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"357\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">15 (bitter)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M286.01,-521.799C299.16,-509.241 317.16,-492.049 331.745,-478.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334.202,-480.613 339.016,-471.175 329.367,-475.551 334.202,-480.613\"/>\n",
       "<text text-anchor=\"middle\" x=\"340\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"157\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (who)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;12 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>15&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.23,-438.197C318.119,-437.084 315.011,-436.004 312,-435 284.78,-425.923 276.561,-427.854 250,-417 230.957,-409.218 210.739,-398.651 194.035,-389.236\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.564,-386.078 185.147,-384.149 192.087,-392.154 195.564,-386.078\"/>\n",
       "<text text-anchor=\"middle\" x=\"265\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\"><title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"235\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">13 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>15&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.248,-434.873C322.846,-429.23 313.495,-422.891 305,-417 292.458,-408.303 278.775,-398.581 266.859,-390.039\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.701,-387.052 258.537,-384.059 264.616,-392.737 268.701,-387.052\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\"><title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">14 (quite)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>15&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M345.397,-434.914C341.84,-429.328 338.06,-423.012 335,-417 331.183,-409.501 327.578,-401.095 324.507,-393.347\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"327.773,-392.088 320.929,-384.001 321.236,-394.592 327.773,-392.088\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node17\" class=\"node\"><title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"405\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">17 (mean)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M368.877,-434.807C372.582,-429.215 376.589,-422.923 380,-417 384.291,-409.549 388.606,-401.267 392.415,-393.622\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"395.681,-394.909 396.928,-384.388 389.392,-391.835 395.681,-394.909\"/>\n",
       "<text text-anchor=\"middle\" x=\"401\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node18\" class=\"node\"><title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"491\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">16 (and)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M387.142,-434.938C396.764,-429.354 407.403,-423.032 417,-417 430.652,-408.419 445.425,-398.558 458.155,-389.873\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"460.335,-392.622 466.602,-384.079 456.376,-386.849 460.335,-392.622\"/>\n",
       "<text text-anchor=\"middle\" x=\"446.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">cc</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\"><title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"364\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">21 (bat)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M396.703,-347.799C390.979,-335.932 383.259,-319.928 376.758,-306.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.782,-304.662 372.285,-297.175 373.477,-307.703 379.782,-304.662\"/>\n",
       "<text text-anchor=\"middle\" x=\"403\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node20\" class=\"node\"><title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"446\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">18 (right)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M413.94,-347.986C416.88,-342.294 420.129,-335.898 423,-330 426.705,-322.389 430.601,-314.058 434.111,-306.416\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"437.343,-307.763 438.307,-297.212 430.974,-304.859 437.343,-307.763\"/>\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node21\" class=\"node\"><title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">19 (off)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>21&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M356.31,-260.799C351.005,-248.932 343.85,-232.928 337.824,-219.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"340.955,-217.876 333.678,-210.175 334.565,-220.733 340.955,-217.876\"/>\n",
       "<text text-anchor=\"middle\" x=\"360\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node22\" class=\"node\"><title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"403\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">20 (the)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;20 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>21&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M371.892,-260.799C377.337,-248.932 384.68,-232.928 390.865,-219.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"394.13,-220.724 395.12,-210.175 387.768,-217.805 394.13,-220.724\"/>\n",
       "<text text-anchor=\"middle\" x=\"394.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node23\" class=\"node\"><title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"486\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">23 (does)</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node24\" class=\"node\"><title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"566\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">24 (n&#39;t)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>25&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M549.811,-521.799C538.1,-509.356 522.108,-492.364 509.063,-478.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"511.568,-476.059 502.165,-471.175 506.47,-480.856 511.568,-476.059\"/>\n",
       "<text text-anchor=\"middle\" x=\"541\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">aux</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;24 -->\n",
       "<g id=\"edge24\" class=\"edge\"><title>25&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M566,-521.799C566,-510.163 566,-494.548 566,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"569.5,-481.175 566,-471.175 562.5,-481.175 569.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"576\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">neg</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\"><title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"649\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">28 (asked)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;28 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>25&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M582.796,-521.799C594.946,-509.356 611.538,-492.364 625.072,-478.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"627.746,-480.775 632.229,-471.175 622.738,-475.885 627.746,-480.775\"/>\n",
       "<text text-anchor=\"middle\" x=\"631.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">ccomp</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\"><title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"572\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">26 (that)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\"><title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M632.431,-434.663C627.14,-429.064 621.287,-422.803 616,-417 608.641,-408.922 600.72,-399.995 593.654,-391.946\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"596.144,-389.476 586.925,-384.254 590.875,-394.085 596.144,-389.476\"/>\n",
       "<text text-anchor=\"middle\" x=\"630.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\"><title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"649\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">27 (I)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\"><title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M649,-434.799C649,-423.163 649,-407.548 649,-394.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"652.5,-394.175 649,-384.175 645.5,-394.175 652.5,-394.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"664\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node29\" class=\"node\"><title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"736\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">31 (address)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;31 -->\n",
       "<g id=\"edge25\" class=\"edge\"><title>28&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M666.606,-434.799C679.341,-422.356 696.733,-405.364 710.919,-391.504\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"713.714,-393.667 718.42,-384.175 708.822,-388.66 713.714,-393.667\"/>\n",
       "<text text-anchor=\"middle\" x=\"713\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\"><title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"676\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">29 (for)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge28\" class=\"edge\"><title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M723.858,-347.799C715.319,-335.702 703.745,-319.305 694.118,-305.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"696.75,-303.327 688.124,-297.175 691.031,-307.364 696.75,-303.327\"/>\n",
       "<text text-anchor=\"middle\" x=\"721\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\"><title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"753\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">30 (his)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge29\" class=\"edge\"><title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M739.44,-347.799C741.767,-336.163 744.89,-320.548 747.553,-307.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"751.036,-307.668 749.565,-297.175 744.172,-306.295 751.036,-307.668\"/>\n",
       "<text text-anchor=\"middle\" x=\"775\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node34\" class=\"node\"><title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"782\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">35 (to)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;35 -->\n",
       "<g id=\"edge34\" class=\"edge\"><title>36&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M782,-521.799C782,-510.163 782,-494.548 782,-481.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"785.5,-481.175 782,-471.175 778.5,-481.175 785.5,-481.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"796.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node35\" class=\"node\"><title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"878\" y=\"-449.3\" font-family=\"Times,serif\" font-size=\"14.00\">38 (account)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge33\" class=\"edge\"><title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M801.427,-521.799C815.61,-509.241 835.027,-492.049 850.758,-478.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"853.435,-480.425 858.602,-471.175 848.795,-475.184 853.435,-480.425\"/>\n",
       "<text text-anchor=\"middle\" x=\"847.5\" y=\"-492.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node36\" class=\"node\"><title>37</title>\n",
       "<text text-anchor=\"middle\" x=\"825\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">37 (the)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;37 -->\n",
       "<g id=\"edge35\" class=\"edge\"><title>38&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M867.275,-434.799C859.732,-422.702 849.508,-406.305 841.004,-392.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"843.97,-390.809 835.709,-384.175 838.03,-394.513 843.97,-390.809\"/>\n",
       "<text text-anchor=\"middle\" x=\"862.5\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node37\" class=\"node\"><title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"908\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">40 (hates)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;40 -->\n",
       "<g id=\"edge36\" class=\"edge\"><title>38&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M884.071,-434.799C888.219,-423.047 893.798,-407.238 898.526,-393.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"901.91,-394.77 901.938,-384.175 895.309,-392.44 901.91,-394.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"906\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node38\" class=\"node\"><title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"991\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">54 (etc)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;54 -->\n",
       "<g id=\"edge37\" class=\"edge\"><title>38&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M900.867,-434.799C917.868,-422.01 941.258,-404.417 959.949,-390.357\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"962.279,-392.984 968.167,-384.175 958.071,-387.39 962.279,-392.984\"/>\n",
       "<text text-anchor=\"middle\" x=\"953\" y=\"-405.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node39\" class=\"node\"><title>50</title>\n",
       "<text text-anchor=\"middle\" x=\"854\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">50 (reaching)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;50 -->\n",
       "<g id=\"edge38\" class=\"edge\"><title>40&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M897.072,-347.799C889.387,-335.702 878.97,-319.305 870.306,-305.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"873.228,-303.739 864.911,-297.175 867.32,-307.493 873.228,-303.739\"/>\n",
       "<text text-anchor=\"middle\" x=\"899\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">advcl</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node40\" class=\"node\"><title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"984\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">43 (has)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;43 -->\n",
       "<g id=\"edge39\" class=\"edge\"><title>40&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M923.38,-347.799C934.402,-335.471 949.417,-318.679 961.745,-304.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"964.587,-306.963 968.643,-297.175 959.369,-302.297 964.587,-306.963\"/>\n",
       "<text text-anchor=\"middle\" x=\"968.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">ccomp</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node48\" class=\"node\"><title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"761\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">49 (before)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;49 -->\n",
       "<g id=\"edge48\" class=\"edge\"><title>50&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M835.18,-260.799C821.44,-248.241 802.63,-231.049 787.39,-217.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"789.535,-214.338 779.792,-210.175 784.812,-219.505 789.535,-214.338\"/>\n",
       "<text text-anchor=\"middle\" x=\"827.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node49\" class=\"node\"><title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"854\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">52 (agent)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge47\" class=\"edge\"><title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M854,-260.799C854,-249.163 854,-233.548 854,-220.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"857.5,-220.175 854,-210.175 850.5,-220.175 857.5,-220.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"866.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">dobj</text>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node41\" class=\"node\"><title>41</title>\n",
       "<text text-anchor=\"middle\" x=\"940\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">41 (that)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;41 -->\n",
       "<g id=\"edge41\" class=\"edge\"><title>43&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M974.607,-260.972C971.507,-255.279 968.068,-248.886 965,-243 961.026,-235.375 956.808,-227.04 952.992,-219.399\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"956.004,-217.595 948.42,-210.197 949.735,-220.71 956.004,-217.595\"/>\n",
       "<text text-anchor=\"middle\" x=\"979.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node42\" class=\"node\"><title>42</title>\n",
       "<text text-anchor=\"middle\" x=\"1018\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">42 (he)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;42 -->\n",
       "<g id=\"edge42\" class=\"edge\"><title>43&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M990.88,-260.799C995.581,-249.047 1001.9,-233.238 1007.26,-219.842\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1010.67,-220.76 1011.13,-210.175 1004.17,-218.16 1010.67,-220.76\"/>\n",
       "<text text-anchor=\"middle\" x=\"1018\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node43\" class=\"node\"><title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"1101\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">45 (speak)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;45 -->\n",
       "<g id=\"edge40\" class=\"edge\"><title>43&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1010.7,-260.961C1019.2,-255.378 1028.58,-249.051 1037,-243 1048.68,-234.604 1061.21,-224.96 1072.06,-216.397\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1074.34,-219.058 1079.99,-210.098 1069.98,-213.578 1074.34,-219.058\"/>\n",
       "<text text-anchor=\"middle\" x=\"1075\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">xcomp</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node44\" class=\"node\"><title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"1065\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">44 (to)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;44 -->\n",
       "<g id=\"edge44\" class=\"edge\"><title>45&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1093.02,-173.984C1090.41,-168.292 1087.53,-161.896 1085,-156 1081.78,-148.471 1078.42,-140.239 1075.39,-132.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1078.58,-131.218 1071.65,-123.209 1072.07,-133.795 1078.58,-131.218\"/>\n",
       "<text text-anchor=\"middle\" x=\"1099.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">mark</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node45\" class=\"node\"><title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"1155\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">48 (machine)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;48 -->\n",
       "<g id=\"edge43\" class=\"edge\"><title>45&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1111.93,-173.799C1119.61,-161.702 1130.03,-145.305 1138.69,-131.667\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1141.68,-133.493 1144.09,-123.175 1135.77,-129.739 1141.68,-133.493\"/>\n",
       "<text text-anchor=\"middle\" x=\"1147\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node46\" class=\"node\"><title>46</title>\n",
       "<text text-anchor=\"middle\" x=\"1115\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">46 (with)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;46 -->\n",
       "<g id=\"edge45\" class=\"edge\"><title>48&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1146.91,-86.799C1141.32,-74.9322 1133.79,-58.9279 1127.45,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1130.51,-43.7333 1123.08,-36.1754 1124.17,-46.7139 1130.51,-43.7333\"/>\n",
       "<text text-anchor=\"middle\" x=\"1149\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node47\" class=\"node\"><title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"1194\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">47 (a)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;47 -->\n",
       "<g id=\"edge46\" class=\"edge\"><title>48&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1162.89,-86.799C1168.34,-74.9322 1175.68,-58.9279 1181.86,-45.4488\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1185.13,-46.724 1186.12,-36.1754 1178.77,-43.8048 1185.13,-46.724\"/>\n",
       "<text text-anchor=\"middle\" x=\"1184.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node50\" class=\"node\"><title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"854\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">51 (an)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;51 -->\n",
       "<g id=\"edge49\" class=\"edge\"><title>52&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M854,-173.799C854,-162.163 854,-146.548 854,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"857.5,-133.175 854,-123.175 850.5,-133.175 857.5,-133.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"862.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fee6d828630>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphviz.Source(list(topPostDepParse[targetSentence])[0].to_dot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these actors are perceive to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "\n",
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [3.4 sec].\n",
      "Adding annotator lemma\n",
      "Adding annotator depparse\n",
      "Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "PreComputed 99996, Elapsed Time: 48.726 (s)\n",
      "Initializing dependency parser ... done [52.8 sec].\n",
      "Adding annotator natlog\n",
      "Adding annotator openie\n",
      "Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0326 seconds]\n",
      "Processing file: /tmp/tmpasi3kz7a\n",
      "All files have been queued; awaiting termination...\n",
      "DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = openIE(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIE()` prints everything stanford core produces and we can see from looking at it that initializing the dependency parser takes most of the time, so calling the function will always take at least 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>elephant</td>\n",
       "      <td>is in</td>\n",
       "      <td>my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant in my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed stimulus efforts in</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview with Wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "      <td>is in</td>\n",
       "      <td>recent interview with Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview with Wall Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>recent interview</td>\n",
       "      <td>is with</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was African American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                      subject                           verb  \\\n",
       "0         1.0                     elephant                          is in   \n",
       "1         1.0                            I                            saw   \n",
       "2         1.0                            I                            saw   \n",
       "3         1.0              quick brown fox                    jumped over   \n",
       "4         1.0              quick brown fox                    jumped over   \n",
       "5         1.0                    quick fox                    jumped over   \n",
       "6         1.0                          fox                    jumped over   \n",
       "7         1.0                    brown fox                    jumped over   \n",
       "8         1.0                    brown fox                    jumped over   \n",
       "9         1.0                    quick fox                    jumped over   \n",
       "10        1.0                          fox                    jumped over   \n",
       "11        1.0            Christine Lagarde                      discussed   \n",
       "12        1.0            Christine Lagarde                      discussed   \n",
       "13        1.0            Christine Lagarde                      discussed   \n",
       "14        1.0            Christine Lagarde  discussed stimulus efforts in   \n",
       "15        1.0            Christine Lagarde                      discussed   \n",
       "16        1.0            Christine Lagarde                      discussed   \n",
       "17        1.0            Christine Lagarde                      discussed   \n",
       "18        1.0  short-term stimulus efforts                          is in   \n",
       "19        1.0            Christine Lagarde                      discussed   \n",
       "20        1.0            Christine Lagarde                      discussed   \n",
       "21        1.0            Christine Lagarde                      discussed   \n",
       "22        1.0            Christine Lagarde                      discussed   \n",
       "23        1.0             recent interview                        is with   \n",
       "24        1.0                       Martin                            was   \n",
       "25        1.0      Trayvon Benjamin Martin      was African American from   \n",
       "26        1.0      Trayvon Benjamin Martin              was American from   \n",
       "27        1.0      Trayvon Benjamin Martin                            was   \n",
       "28        1.0      Trayvon Benjamin Martin                            was   \n",
       "\n",
       "                                               object  \n",
       "0                                          my pajamas  \n",
       "1                              elephant in my pajamas  \n",
       "2                                            elephant  \n",
       "3                                            lazy dog  \n",
       "4                                                 dog  \n",
       "5                                                 dog  \n",
       "6                                                 dog  \n",
       "7                                            lazy dog  \n",
       "8                                                 dog  \n",
       "9                                            lazy dog  \n",
       "10                                           lazy dog  \n",
       "11  short-term stimulus efforts in interview with ...  \n",
       "12                      stimulus efforts in interview  \n",
       "13               stimulus efforts in recent interview  \n",
       "14                                             France  \n",
       "15  short-term stimulus efforts in recent intervie...  \n",
       "16  stimulus efforts in recent interview with Wall...  \n",
       "17    short-term stimulus efforts in recent interview  \n",
       "18          recent interview with Wall Street Journal  \n",
       "19  stimulus efforts in interview with Wall Street...  \n",
       "20                                   stimulus efforts  \n",
       "21                        short-term stimulus efforts  \n",
       "22           short-term stimulus efforts in interview  \n",
       "23                                Wall Street Journal  \n",
       "24                                            African  \n",
       "25                                            Florida  \n",
       "26                                            Florida  \n",
       "27                                           American  \n",
       "28                                   African American  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No buffalos (because there were no verbs), but the rest is somewhat promising. Note, however, that it abandoned the key theme of the sentence about the tragic Trayvon Martin death (\"fatally shot\"), likely because it was buried so deeply within the complex phrase structure. This is obviously a challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your thoughts*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also look for subject, object, target triples in one of the reddit stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.5 sec].\n",
      "Adding annotator lemma\n",
      "Adding annotator depparse\n",
      "Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "PreComputed 99996, Elapsed Time: 43.835 (s)\n",
      "Initializing dependency parser ... done [47.7 sec].\n",
      "Adding annotator natlog\n",
      "Adding annotator openie\n",
      "Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [2.0109 seconds]\n",
      "Processing file: /tmp/tmpy6dpih7k\n",
      "All files have been queued; awaiting termination...\n",
      "DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = openIE(redditTopScores['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>Quite often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>many homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our digital equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>repeat offenders</td>\n",
       "      <td>is with</td>\n",
       "      <td>long ticket histories of types of issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>speak with</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>So anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak with machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>however was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.780294</td>\n",
       "      <td>handling</td>\n",
       "      <td>types of</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy again for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our front entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put to</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>was</td>\n",
       "      <td>framed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>just going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty                subject                            verb  \\\n",
       "0     1.000000                     we                         'll get   \n",
       "1     1.000000                     we             Quite often 'll get   \n",
       "2     1.000000                     we                   often 'll get   \n",
       "3     0.831036                     we                            coax   \n",
       "4     0.774359  straight analog cable                            coax   \n",
       "5     0.774359           analog cable                            coax   \n",
       "6     0.774359  straight analog cable                            coax   \n",
       "7     1.000000                     we    would supply analog cable to   \n",
       "8     0.831036                     we                            coax   \n",
       "9     0.774359           analog cable                            coax   \n",
       "10    0.831036                     we                            coax   \n",
       "11    0.774359  straight analog cable                            coax   \n",
       "12    0.774359  straight analog cable                            coax   \n",
       "13    0.831036                     we                            coax   \n",
       "14    0.774359           analog cable                            coax   \n",
       "15    1.000000                     we    would supply analog cable to   \n",
       "16    0.774359           analog cable                            coax   \n",
       "17    1.000000          our equipment                         receive   \n",
       "18    1.000000  our digital equipment                         receive   \n",
       "19    1.000000       repeat offenders                         is with   \n",
       "20    1.000000                      I                      anyway get   \n",
       "21    1.000000                      I                             get   \n",
       "22    1.000000                     he                      speak with   \n",
       "23    1.000000                      I                   So anyway get   \n",
       "24    1.000000                     he                             has   \n",
       "25    1.000000                     he                             has   \n",
       "26    1.000000                   call               however was going   \n",
       "27    1.000000                   call                       was going   \n",
       "28    0.780294               handling                        types of   \n",
       "29    1.000000                   call                       was going   \n",
       "..         ...                    ...                             ...   \n",
       "161   1.000000                     it                            made   \n",
       "162   1.000000                     it                            made   \n",
       "163   1.000000                     it                     really made   \n",
       "164   1.000000                    man                       happy for   \n",
       "165   1.000000                old man                 happy again for   \n",
       "166   1.000000                    man                 happy again for   \n",
       "167   1.000000                     it                     really made   \n",
       "168   1.000000                old man                 happy again for   \n",
       "169   1.000000                     it                     really made   \n",
       "170   1.000000                    man                       happy for   \n",
       "171   1.000000                     it                            made   \n",
       "172   1.000000                     it                            made   \n",
       "173   1.000000                     it                     really made   \n",
       "174   1.000000                     it                     really made   \n",
       "175   1.000000                old man                       happy for   \n",
       "176   1.000000                     it                     really made   \n",
       "177   1.000000                     it                            made   \n",
       "178   1.000000                     it                            made   \n",
       "179   1.000000                     it                     really made   \n",
       "180   1.000000                     it                            made   \n",
       "181   1.000000                 letter                          put on   \n",
       "182   1.000000                 letter                          put to   \n",
       "183   1.000000                 letter                          put on   \n",
       "184   1.000000                 letter                             was   \n",
       "185   1.000000                   they                   going through   \n",
       "186   1.000000                   they              just going through   \n",
       "187   1.000000                      I               still think about   \n",
       "188   1.000000                      I  still think occasionally about   \n",
       "189   1.000000                      I        think occasionally about   \n",
       "190   1.000000                      I                     think about   \n",
       "\n",
       "                                             object  \n",
       "0                                             calls  \n",
       "1                                             calls  \n",
       "2                                             calls  \n",
       "3                                      direct to TV  \n",
       "4                                  direct from wall  \n",
       "5                            direct from wall to TV  \n",
       "6                                      direct to TV  \n",
       "7                                             homes  \n",
       "8                                  direct from wall  \n",
       "9                                  direct from wall  \n",
       "10                           direct from wall to TV  \n",
       "11                           direct from wall to TV  \n",
       "12                                           direct  \n",
       "13                                           direct  \n",
       "14                                     direct to TV  \n",
       "15                                       many homes  \n",
       "16                                           direct  \n",
       "17                                         channels  \n",
       "18                                         channels  \n",
       "19         long ticket histories of types of issues  \n",
       "20                                             call  \n",
       "21                                             call  \n",
       "22                                          machine  \n",
       "23                                             call  \n",
       "24                                            speak  \n",
       "25                               speak with machine  \n",
       "26                                 little different  \n",
       "27                                 little different  \n",
       "28                                        customers  \n",
       "29                                        different  \n",
       "..                                              ...  \n",
       "161                                       man happy  \n",
       "162           man happy again for once in long time  \n",
       "163                  old man happy for once in time  \n",
       "164                                            once  \n",
       "165                                    once in time  \n",
       "166                                            once  \n",
       "167                                   old man happy  \n",
       "168                          once in very long time  \n",
       "169                                 man happy again  \n",
       "170                               once in long time  \n",
       "171            old man happy again for once in time  \n",
       "172                        man happy again for once  \n",
       "173  old man happy again for once in very long time  \n",
       "174       old man happy again for once in long time  \n",
       "175                                    once in time  \n",
       "176                      man happy for once in time  \n",
       "177            man happy for once in very long time  \n",
       "178        old man happy for once in very long time  \n",
       "179           man happy again for once in long time  \n",
       "180      man happy again for once in very long time  \n",
       "181                              our front entrance  \n",
       "182                                          retail  \n",
       "183                                    our entrance  \n",
       "184                                          framed  \n",
       "185                                             lot  \n",
       "186                                             lot  \n",
       "187                                       Mr. Smith  \n",
       "188                                       Mr. Smith  \n",
       "189                                       Mr. Smith  \n",
       "190                                       Mr. Smith  \n",
       "\n",
       "[191 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 200 triples in only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redditTopScores['sentences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in redditTopScores['sentences'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find at the most common subject in this story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I                        48\n",
       "it                       42\n",
       "he                       19\n",
       "He                       18\n",
       "we                       11\n",
       "man                       8\n",
       "old man                   8\n",
       "letter                    4\n",
       "our booking calendar      4\n",
       "call                      4\n",
       "analog cable              4\n",
       "straight analog cable     4\n",
       "my supervisor             3\n",
       "you                       2\n",
       "they                      2\n",
       "TV                        2\n",
       "his TV set                2\n",
       "people                    1\n",
       "handling                  1\n",
       "me                        1\n",
       "our equipment             1\n",
       "our digital equipment     1\n",
       "repeat offenders          1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I is followed by various male pronouns and compound nouns (e.g., \"old man\"). 'I' occures most often with the following verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "could come                        8\n",
       "brought                           5\n",
       "even brought                      5\n",
       "was                               4\n",
       "had                               4\n",
       "speak for                         3\n",
       "So anyway get                     1\n",
       "felt                              1\n",
       "still think about                 1\n",
       "instantly felt                    1\n",
       "had cable within                  1\n",
       "get to                            1\n",
       "complaint in                      1\n",
       "anyway get                        1\n",
       "think about                       1\n",
       "get                               1\n",
       "do                                1\n",
       "think occasionally about          1\n",
       "eventually had                    1\n",
       "took                              1\n",
       "have                              1\n",
       "still think occasionally about    1\n",
       "speak with                        1\n",
       "ask                               1\n",
       "'ve dealt with                    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr. Smith                                             4\n",
       "him                                                   3\n",
       "call                                                  3\n",
       "simplified remote                                     2\n",
       "get                                                   2\n",
       "this                                                  2\n",
       "willing                                               2\n",
       "simplified remote for his set top box                 2\n",
       "bad                                                   2\n",
       "remote                                                2\n",
       "remote for his set top box                            2\n",
       "cable                                                 1\n",
       "speak with her for bit about account for Mr. Smith    1\n",
       "bit about account for Mr. Smith                       1\n",
       "book                                                  1\n",
       "bit about account                                     1\n",
       "how useless                                           1\n",
       "speak with her for bit                                1\n",
       "speak for bit about account for Mr. Smith             1\n",
       "cable running                                         1\n",
       "speak for bit                                         1\n",
       "speak with her                                        1\n",
       "it                                                    1\n",
       "bit                                                   1\n",
       "her                                                   1\n",
       "cable running again                                   1\n",
       "experience                                            1\n",
       "speak for bit about account                           1\n",
       "residence                                             1\n",
       "speak with her for bit about account                  1\n",
       "30 seconds                                            1\n",
       "speak                                                 1\n",
       "useless                                               1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa? Describe how you would select among these statements to create a database of high-value statements for your project and do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing POS\n",
    "\n",
    "We can check the POS tagger by running it on an already tagged corpus and counting the number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank = nltk.corpus.treebank\n",
    "treeBank.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stanfordTags = postTagger.tag_sents(treeBank.sents()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Dutch  \tStanford: JJ\tTreebank: NNP\n",
      "Word: publishing  \tStanford: NN\tTreebank: VBG\n",
      "Word: used  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: later  \tStanford: RB\tTreebank: JJ\n",
      "Word: New  \tStanford: NNP\tTreebank: JJ\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: replaced  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: JJ\n",
      "Word: expected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: study  \tStanford: VBD\tTreebank: VBP\n",
      "Word: studied  \tStanford: VBD\tTreebank: VBN\n",
      "Word: industrialized  \tStanford: JJ\tTreebank: VBN\n",
      "Word: Lorillard  \tStanford: NNP\tTreebank: NN\n",
      "Word: found  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: rejected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: poured  \tStanford: VBN\tTreebank: VBD\n",
      "Word: in  \tStanford: IN\tTreebank: RP\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "The difference rate is 3.453%\n"
     ]
    }
   ],
   "source": [
    "NumDiffs = 0\n",
    "for sentIndex in range(len(stanfordTags)):\n",
    "    for wordIndex in range(len(stanfordTags[sentIndex])):\n",
    "        if stanfordTags[sentIndex][wordIndex][1] != treeBank.tagged_sents()[sentIndex][wordIndex][1]:\n",
    "            if treeBank.tagged_sents()[sentIndex][wordIndex][1] != '-NONE-':\n",
    "                print(\"Word: {}  \\tStanford: {}\\tTreebank: {}\".format(stanfordTags[sentIndex][wordIndex][0], stanfordTags[sentIndex][wordIndex][1], treeBank.tagged_sents()[sentIndex][wordIndex][1]))\n",
    "                NumDiffs += 1\n",
    "total = sum([len(s) for s in stanfordTags])\n",
    "print(\"The difference rate is {:.3f}%\".format(NumDiffs /total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the stanford POS tagger is very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
