{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layout\n",
    "\n",
    "+ opening\n",
    "    + contents\n",
    "    + files used\n",
    "    + packages used\n",
    "+ Scraping\n",
    "    + Images\n",
    "        + OCR\n",
    "    + raw text\n",
    "    + html\n",
    "    + PDFs\n",
    "    + word doc etc.\n",
    "+ spidering\n",
    "    + wikipedia\n",
    "    + APIs\n",
    "        + REST\n",
    "        + tumblr\n",
    "            + Newly registered consumers are rate limited to 250 requests per hour, and 5,000 requests per day. If your application requires more requests for either of these periods, please use the 'Request rate limit removal' link on an app above.\n",
    "            + Reid McIlroy-Young OAuth\n",
    "                + Consumer Key: TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8\n",
    "                + Secret Key:  GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv\n",
    "+ reading files\n",
    "    + encodings\n",
    "    + unicode\n",
    "+ filtering\n",
    "+ data structures\n",
    "    + pandas\n",
    "\n",
    "\n",
    "# Week 1 - Intro\n",
    "\n",
    "Intro stuff ...\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import bs4 #called 'BeautifulSoup', an html parser\n",
    "import pandas #gives us DataFrames\n",
    "import docx #reading MS doc files, install as `python-docx`\n",
    "\n",
    "#Stuff for pdfs\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "#These come with Python\n",
    "import re #for regexs\n",
    "import urllib.parse #For joining urls\n",
    "import io #for making http requests look like files\n",
    "import json #For Tumblr api responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be working on the following files/urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "example_text_file = 'sometextfile.txt'\n",
    "information_extraction_pdf = 'https://web.stanford.edu/~jurafsky/slp3/20.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Before we can start analyzing content we need to obtain it. Sometimes it will be provided to us before hand, but often we will need to download it. As a starting example we will attempt to download the wikipedia page on content analysis. The page is located at [https://en.wikipedia.org/wiki/Content_analysis](https://en.wikipedia.org/wiki/Content_analysis) so lets start with that.\n",
    "\n",
    "We can do this by making an HTTP GET request to that url, a GET request is simply a request to the server to provide the contents given by some url. The other request we will be using in this class is called a POST request and requests the server to take some content we provide. While the Python standard library does have the ability do make GET requests we will be using the [_requests_](http://docs.python-requests.org/en/master/) package as it is _'the only Non-GMO HTTP library for Python'_, also it provides a nicer interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "requests.get('https://en.wikipedia.org/wiki/Content_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Response [200]'` means the server responded with what we asked for. If you get another number (e.g. 404) it likely means there was some kind of error, these codes are called HTTP response codes and a list of them can be found [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). The response object contains all the data the server sent including the website's contents and the HTTP header. We are interested in the contents which we can access with the `.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Content analysis - Wikipedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n",
      "<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Content_analysis\",\"wgTitle\":\"Content analysis\",\"wgCurRevisionId\":735443188,\"wgRevisionId\":735443188,\"wgArticleId\":473317,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles needing cleanup from April 2008\",\"All articles needing cleanup\",\"Cleanup tagged articles without a reason field from April 2008\",\"Wikipedia pages needing cleanup from April 2008\",\"Articles needing expert attention with no reason or talk parameter\",\"Articles needing expert attention from April 2008\",\"All artic\n"
     ]
    }
   ],
   "source": [
    "wikiContentRequest = requests.get('https://en.wikipedia.org/wiki/Content_analysis')\n",
    "print(wikiContentRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we were looking for, because it is the start of the HTML that makes up the website. This is HTML and is meant to be read by computers. Luckily we have a computer to parse it for us. To do the parsing we will use [_Beautiful Soup_](https://www.crummy.com/software/BeautifulSoup/) which is a better parser than the one in the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Content analysis - Wikipedia\n",
      "document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
      "(window.RLQ=window.RLQ||[]).push(functio\n"
     ]
    }
   ],
   "source": [
    "wikiContentSoup = bs4.BeautifulSoup(wikiContentRequest.text, 'html.parser')\n",
    "print(wikiContentSoup.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better but there's a bunch of random whitespace and we have way more than just the text of the article. This is because what we requested is the whole webpage, not just the text for the article.\n",
    "\n",
    "We need to extract only the text we care about, in order to do this we will need to inspect the html. One way to do this is to simply go to the website with a browser and use its inspection or view source tool, but if there is javascript or other dynamic loading occurring on the page it is very likely that what Python receives is not what you will see. So we will need to view what Python receives. To do this we can save the html `requests` obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "\n",
    "with open(content_analysis_save, mode='w', encoding='utf-8') as f:\n",
    "    f.write(wikiContentRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets open the file (`wikipedia_content_analysis.html`) we just created with a web browser. It should look sort of like the original but missing all the images and formatting.\n",
    "\n",
    "As there is very little standardization on structuring webpages figuring out how best to extract what you want is an art. Looking at this page it looks like all the main textual content is within `<p>`(paragraph) tags inside the `<body>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content analysis is \"a wide and heterogeneous set of manual or computer-assisted techniques for contextualized interpretations of documents produced by communication processes in the strict sense of that phrase (any kind of text, written, iconic, multimedia, etc.) or signification processes (traces and artifacts), having as ultimate goal the production of valid and trustworthy inferences.\"\n",
      "Content analysis has come to be a sort of 'umbrella term' referring to an almost boundless set of quite diverse research approaches and techniques. Broadly, it can refer to methods for studying and/or retrieving meaningful information from documents.[1] In a more focused way, content analysis refers to a family of techniques for studying the \"mute evidence\" of texts and artifacts.[2] There are 5 types of texts in content analysis:\n",
      "Content analysis can also be described as studying traces, which are documents from past times, and artifacts, which are non-linguistic documents. Texts are understood to be produced by communication processes in a broad sense of that phrase - often gaining mean through abduction.[1][3]\n"
     ]
    }
   ],
   "source": [
    "contentPTags = wikiContentSoup.body.findAll('p')\n",
    "for pTag in contentPTags[:3]:\n",
    "    print(pTag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the text from the page, split up by paragraph. If we wanted to get the section headers or references as well it would require a bit more work, but is doable.\n",
    "\n",
    "There is one more thing we might want to do before sending this text to be processed, remove the references indicators (`[2]`, `[3]` , etc). To do this we can use a short regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       paragraph-text\n",
      "0   Content analysis is \"a wide and heterogeneous ...\n",
      "1   Content analysis has come to be a sort of 'umb...\n",
      "2   Content analysis can also be described as stud...\n",
      "3   Despite the wide variety of options, generally...\n",
      "4   Over the years, content analysis has been appl...\n",
      "5   In recent times, particularly with the advent ...\n",
      "6   Quantitative content analysis has enjoyed a re...\n",
      "7                                                    \n",
      "8                                                    \n",
      "9   The method of content analysis enables the res...\n",
      "10  Since the 1980s, content analysis has become a...\n",
      "11  The creation of coding frames is intrinsically...\n",
      "12  Mimetic Convergence thus aims to show the proc...\n",
      "13  Every content analysis should depart from a hy...\n",
      "14  As an evaluation approach, content analysis is...\n",
      "15  Qualitative content analysis is “a systematic,...\n",
      "16  Holsti groups fifteen uses of content analysis...\n",
      "17  He also places these uses into the context of ...\n",
      "18  The following table shows fifteen uses of cont...\n",
      "19  According to Dr. Klaus Krippendorff, six quest...\n",
      "20  The assumption is that words and phrases menti...\n",
      "21  Qualitatively, content analysis can involve an...\n",
      "22  Normally, content analysis can only be applied...\n",
      "23  A further step in analysis is the distinction ...\n",
      "24  Dermot McKeone highlighted the difference betw...\n",
      "25  As the uncritical use of text is today widely ...\n",
      "26  Neuendorf suggests that when human coders are ...\n"
     ]
    }
   ],
   "source": [
    "contentParagraphs = []\n",
    "for pTag in contentPTags:\n",
    "    #strings starting with r are raw so their \\'s are not modifier characters\n",
    "    #If we didn't start with r the string would be: '\\\\[\\\\d+\\\\]'\n",
    "    contentParagraphs.append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "\n",
    "#convert to a DataFrame\n",
    "contentParagraphsDF = pandas.DataFrame({'paragraph-text' : contentParagraphs})\n",
    "print(contentParagraphsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `DataFrame` of all the relevant text from the page ready to be processed\n",
    "\n",
    "If you are not familiar with regex, it is a way of specifying searches of text. A regex engine takes in the search pattern, in the above case `'\\[\\d+\\]'` and some string, the paragraph texts. Then it reads the input string one character at a time checking if it matches the search. For example the regex `'\\d'` matches number characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(36, 37), match='2'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findNumber = r'\\d'\n",
    "regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "regexResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python the regex package (`re`) usually returns `Match` objects (you can have multiple pattern hits in a a single `Match`), to get the string that matched our pattern we can use the `.group()` method, and as we want the first one will will ask for the 0'th group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us the first number, if we wanted the whole block of numbers we can add a wildcard `'+'` which requests 1 or more instances of the proceeding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2134567890\n"
     ]
    }
   ],
   "source": [
    "findNumbers = r'\\d+'\n",
    "regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the whole block of numbers, there are a huge number of special characters in regex, for the full description of Python's implementation look at the [re docs](https://docs.python.org/3/library/re.html) there is also a short [tutorial](https://docs.python.org/3/howto/regex.html#regex-howto).\n",
    "\n",
    "# Spidering\n",
    "\n",
    "What if we want to to get a bunch of different pages from wikipedia. We would need to get the url of each of the pages we want, usually we will want pages that are linked to by other pages, so we will need to parse pages and find the links. Right now we will be getting all the links in the body of the content analysis page.\n",
    "\n",
    "To do this we will need to find all the `<a>` (anchor) tags with `href`s (hyperlink references) inside of `<p>` tags. `href` can have many [different](http://stackoverflow.com/questions/4855168/what-is-href-and-why-is-it-used) [forms](https://en.wikipedia.org/wiki/Hyperlink#Hyperlinks_in_HTML) so dealing with them can be tricky generally though you be extracting from it absolute or relative links. An absolute link is one you can follow with out any modification, while a relative link has a base url that you are then appending. Wikipedia uses relative urls for its internal links so below is an example of dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://en.wikipedia.org/wiki/Text_(literary_theory)', 1, 'texts'), ('https://en.wikipedia.org/wiki/Trace_evidence', 2, 'traces'), ('https://en.wikipedia.org/wiki/Abductive_reasoning', 2, 'abduction'), ('https://en.wikipedia.org/wiki/Hermeneutics', 4, 'Hermeneutics'), ('https://en.wikipedia.org/wiki/Philology', 4, 'Philology'), ('https://en.wikipedia.org/wiki/Authentication', 4, 'authenticity'), ('https://en.wikipedia.org/wiki/Mass_communication', 5, 'mass communication'), ('https://en.wikipedia.org/wiki/Harold_Lasswell', 5, 'Harold Lasswell'), ('https://en.wikipedia.org/wiki/Bernard_Berelson', 5, 'Bernard Berelson'), ('https://en.wikipedia.org/wiki/Big_data', 6, 'big data')]\n"
     ]
    }
   ],
   "source": [
    "#wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "otherPAgeURLS = []\n",
    "#We also want to know where the links come from so we also will get:\n",
    "#the paragraph number\n",
    "#the word the link is in\n",
    "for paragraphNum, pTag in enumerate(contentPTags):\n",
    "    #we only want hrefs that link to wiki pages\n",
    "    tagLinks = pTag.findAll('a', href=re.compile('/wiki/'), class_=False)\n",
    "    for aTag in tagLinks:\n",
    "        #We need to extract the url from the <a> tag\n",
    "        relurl = aTag.get('href')\n",
    "        linkText = aTag.text\n",
    "        #wikipedia_base_url is the base we can use the urllib joining function to merge them\n",
    "        #Giving a nice structured tupe like this means we can use tuple expansion later\n",
    "        otherPAgeURLS.append((\n",
    "            urllib.parse.urljoin(wikipedia_base_url, relurl),\n",
    "            paragraphNum,\n",
    "            linkText,\n",
    "        ))\n",
    "print(otherPAgeURLS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be adding these new texts to our DataFrame `contentParagraphsDF` so we will need to add 2 more columns to keep track of paragraph numbers and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>paragraph-number</th>\n",
       "      <th>source-paragraph-number</th>\n",
       "      <th>source-paragraph-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Content analysis is \"a wide and heterogeneous ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Content analysis has come to be a sort of 'umb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Content analysis can also be described as stud...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Despite the wide variety of options, generally...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over the years, content analysis has been appl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In recent times, particularly with the advent ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quantitative content analysis has enjoyed a re...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The method of content analysis enables the res...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Since the 1980s, content analysis has become a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The creation of coding frames is intrinsically...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mimetic Convergence thus aims to show the proc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Every content analysis should depart from a hy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>As an evaluation approach, content analysis is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qualitative content analysis is “a systematic,...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Holsti groups fifteen uses of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>16</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>He also places these uses into the context of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The following table shows fifteen uses of cont...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>According to Dr. Klaus Krippendorff, six quest...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The assumption is that words and phrases menti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Qualitatively, content analysis can involve an...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>21</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Normally, content analysis can only be applied...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A further step in analysis is the distinction ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Dermot McKeone highlighted the difference betw...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>As the uncritical use of text is today widely ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Neuendorf suggests that when human coders are ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>26</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paragraph-text  \\\n",
       "0   Content analysis is \"a wide and heterogeneous ...   \n",
       "1   Content analysis has come to be a sort of 'umb...   \n",
       "2   Content analysis can also be described as stud...   \n",
       "3   Despite the wide variety of options, generally...   \n",
       "4   Over the years, content analysis has been appl...   \n",
       "5   In recent times, particularly with the advent ...   \n",
       "6   Quantitative content analysis has enjoyed a re...   \n",
       "7                                                       \n",
       "8                                                       \n",
       "9   The method of content analysis enables the res...   \n",
       "10  Since the 1980s, content analysis has become a...   \n",
       "11  The creation of coding frames is intrinsically...   \n",
       "12  Mimetic Convergence thus aims to show the proc...   \n",
       "13  Every content analysis should depart from a hy...   \n",
       "14  As an evaluation approach, content analysis is...   \n",
       "15  Qualitative content analysis is “a systematic,...   \n",
       "16  Holsti groups fifteen uses of content analysis...   \n",
       "17  He also places these uses into the context of ...   \n",
       "18  The following table shows fifteen uses of cont...   \n",
       "19  According to Dr. Klaus Krippendorff, six quest...   \n",
       "20  The assumption is that words and phrases menti...   \n",
       "21  Qualitatively, content analysis can involve an...   \n",
       "22  Normally, content analysis can only be applied...   \n",
       "23  A further step in analysis is the distinction ...   \n",
       "24  Dermot McKeone highlighted the difference betw...   \n",
       "25  As the uncritical use of text is today widely ...   \n",
       "26  Neuendorf suggests that when human coders are ...   \n",
       "\n",
       "                                            source  paragraph-number  \\\n",
       "0   https://en.wikipedia.org/wiki/Content_analysis                 0   \n",
       "1   https://en.wikipedia.org/wiki/Content_analysis                 1   \n",
       "2   https://en.wikipedia.org/wiki/Content_analysis                 2   \n",
       "3   https://en.wikipedia.org/wiki/Content_analysis                 3   \n",
       "4   https://en.wikipedia.org/wiki/Content_analysis                 4   \n",
       "5   https://en.wikipedia.org/wiki/Content_analysis                 5   \n",
       "6   https://en.wikipedia.org/wiki/Content_analysis                 6   \n",
       "7   https://en.wikipedia.org/wiki/Content_analysis                 7   \n",
       "8   https://en.wikipedia.org/wiki/Content_analysis                 8   \n",
       "9   https://en.wikipedia.org/wiki/Content_analysis                 9   \n",
       "10  https://en.wikipedia.org/wiki/Content_analysis                10   \n",
       "11  https://en.wikipedia.org/wiki/Content_analysis                11   \n",
       "12  https://en.wikipedia.org/wiki/Content_analysis                12   \n",
       "13  https://en.wikipedia.org/wiki/Content_analysis                13   \n",
       "14  https://en.wikipedia.org/wiki/Content_analysis                14   \n",
       "15  https://en.wikipedia.org/wiki/Content_analysis                15   \n",
       "16  https://en.wikipedia.org/wiki/Content_analysis                16   \n",
       "17  https://en.wikipedia.org/wiki/Content_analysis                17   \n",
       "18  https://en.wikipedia.org/wiki/Content_analysis                18   \n",
       "19  https://en.wikipedia.org/wiki/Content_analysis                19   \n",
       "20  https://en.wikipedia.org/wiki/Content_analysis                20   \n",
       "21  https://en.wikipedia.org/wiki/Content_analysis                21   \n",
       "22  https://en.wikipedia.org/wiki/Content_analysis                22   \n",
       "23  https://en.wikipedia.org/wiki/Content_analysis                23   \n",
       "24  https://en.wikipedia.org/wiki/Content_analysis                24   \n",
       "25  https://en.wikipedia.org/wiki/Content_analysis                25   \n",
       "26  https://en.wikipedia.org/wiki/Content_analysis                26   \n",
       "\n",
       "   source-paragraph-number source-paragraph-text  \n",
       "0                     None                  None  \n",
       "1                     None                  None  \n",
       "2                     None                  None  \n",
       "3                     None                  None  \n",
       "4                     None                  None  \n",
       "5                     None                  None  \n",
       "6                     None                  None  \n",
       "7                     None                  None  \n",
       "8                     None                  None  \n",
       "9                     None                  None  \n",
       "10                    None                  None  \n",
       "11                    None                  None  \n",
       "12                    None                  None  \n",
       "13                    None                  None  \n",
       "14                    None                  None  \n",
       "15                    None                  None  \n",
       "16                    None                  None  \n",
       "17                    None                  None  \n",
       "18                    None                  None  \n",
       "19                    None                  None  \n",
       "20                    None                  None  \n",
       "21                    None                  None  \n",
       "22                    None                  None  \n",
       "23                    None                  None  \n",
       "24                    None                  None  \n",
       "25                    None                  None  \n",
       "26                    None                  None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentParagraphsDF['source'] = [wikipedia_content_analysis] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['paragraph-number'] = range(len(contentParagraphsDF['paragraph-text']))\n",
    "contentParagraphsDF['source-paragraph-number'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['source-paragraph-text'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define a function to parse each linked page and add its text to our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextFromWikiPage(targetURL, sourceParNum, sourceText):\n",
    "    #Make a dict to store data before adding it to the DataFrame\n",
    "    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}\n",
    "    #Now we get the page\n",
    "    r = requests.get(targetURL)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #enumerating gives use the paragraph number\n",
    "    for parNum, pTag in enumerate(soup.body.findAll('p')):\n",
    "        #same regex as before\n",
    "        parsDict['paragraph-text'].append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "        parsDict['paragraph-number'].append(parNum)\n",
    "        parsDict['source'].append(targetURL)\n",
    "        parsDict['source-paragraph-number'].append(sourceParNum)\n",
    "        parsDict['source-paragraph-text'].append(sourceText)\n",
    "    return pandas.DataFrame(parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run it on our list of link tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-number</th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>source-paragraph-number</th>\n",
       "      <th>source-paragraph-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Content analysis is \"a wide and heterogeneous ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Content analysis has come to be a sort of 'umb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Content analysis can also be described as stud...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Despite the wide variety of options, generally...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Over the years, content analysis has been appl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>In recent times, particularly with the advent ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Quantitative content analysis has enjoyed a re...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>The method of content analysis enables the res...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Since the 1980s, content analysis has become a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>The creation of coding frames is intrinsically...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Mimetic Convergence thus aims to show the proc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Every content analysis should depart from a hy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>As an evaluation approach, content analysis is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Qualitative content analysis is “a systematic,...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Holsti groups fifteen uses of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>He also places these uses into the context of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>The following table shows fifteen uses of cont...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>According to Dr. Klaus Krippendorff, six quest...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>The assumption is that words and phrases menti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Qualitatively, content analysis can involve an...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Normally, content analysis can only be applied...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>A further step in analysis is the distinction ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Dermot McKeone highlighted the difference betw...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>As the uncritical use of text is today widely ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Neuendorf suggests that when human coders are ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>In literary theory, a text is any object that ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Text_(literary_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>texts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Within the field of literary criticism, \"text\"...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Text_(literary_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>texts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>Since the history of writing predates the conc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Text_(literary_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>texts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>53</td>\n",
       "      <td>The hypothesis is framed, but not asserted, in...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>54</td>\n",
       "      <td>Note that the hypothesis (\"A\") could be of a r...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>55</td>\n",
       "      <td>Peirce did not remain quite convinced about an...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>56</td>\n",
       "      <td>In 1901 Peirce wrote, \"There would be no logic...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>57</td>\n",
       "      <td>\"Consider what effects, that might conceivably...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>58</td>\n",
       "      <td>It is a method for fruitful clarification of c...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>59</td>\n",
       "      <td>Peirce came over the years to divide (philosop...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>60</td>\n",
       "      <td>Peirce had, from the start, seen the modes of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>61</td>\n",
       "      <td>As early as 1866, Peirce held that:</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>62</td>\n",
       "      <td>1. Hypothesis (abductive inference) is inferen...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>63</td>\n",
       "      <td>In 1902, Peirce wrote that, in abduction: \"It ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>64</td>\n",
       "      <td>At the critical level Peirce examined the form...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>65</td>\n",
       "      <td>The phrase \"inference to the best explanation\"...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>66</td>\n",
       "      <td>At the methodeutical level Peirce held that a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>67</td>\n",
       "      <td>Norwood Russell Hanson, a philosopher of scien...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>68</td>\n",
       "      <td>Further development of the concept can be foun...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>69</td>\n",
       "      <td>Applications in artificial intelligence includ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>70</td>\n",
       "      <td>In medicine, abduction can be seen as a compon...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>71</td>\n",
       "      <td>Abduction can also be used to model automated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>72</td>\n",
       "      <td>In intelligence analysis, analysis of competin...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>73</td>\n",
       "      <td>Belief revision, the process of adapting belie...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>74</td>\n",
       "      <td>In the philosophy of science, abduction has be...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>75</td>\n",
       "      <td>In historical linguistics, abduction during la...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>76</td>\n",
       "      <td>In anthropology, Alfred Gell in his influentia...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>77</td>\n",
       "      <td>Consequently, to discover is simply to expedit...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>78</td>\n",
       "      <td>It allows any flight of imagination, provided ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>79</td>\n",
       "      <td>Methodeutic has a special interest in Abductio...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>80</td>\n",
       "      <td>.... What is good abduction? What should an ex...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>81</td>\n",
       "      <td>The mind seeks to bring the facts, as modified...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>82</td>\n",
       "      <td>Thus, twenty skillful hypotheses will ascertai...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     paragraph-number                                     paragraph-text  \\\n",
       "0                   0  Content analysis is \"a wide and heterogeneous ...   \n",
       "1                   1  Content analysis has come to be a sort of 'umb...   \n",
       "2                   2  Content analysis can also be described as stud...   \n",
       "3                   3  Despite the wide variety of options, generally...   \n",
       "4                   4  Over the years, content analysis has been appl...   \n",
       "5                   5  In recent times, particularly with the advent ...   \n",
       "6                   6  Quantitative content analysis has enjoyed a re...   \n",
       "7                   7                                                      \n",
       "8                   8                                                      \n",
       "9                   9  The method of content analysis enables the res...   \n",
       "10                 10  Since the 1980s, content analysis has become a...   \n",
       "11                 11  The creation of coding frames is intrinsically...   \n",
       "12                 12  Mimetic Convergence thus aims to show the proc...   \n",
       "13                 13  Every content analysis should depart from a hy...   \n",
       "14                 14  As an evaluation approach, content analysis is...   \n",
       "15                 15  Qualitative content analysis is “a systematic,...   \n",
       "16                 16  Holsti groups fifteen uses of content analysis...   \n",
       "17                 17  He also places these uses into the context of ...   \n",
       "18                 18  The following table shows fifteen uses of cont...   \n",
       "19                 19  According to Dr. Klaus Krippendorff, six quest...   \n",
       "20                 20  The assumption is that words and phrases menti...   \n",
       "21                 21  Qualitatively, content analysis can involve an...   \n",
       "22                 22  Normally, content analysis can only be applied...   \n",
       "23                 23  A further step in analysis is the distinction ...   \n",
       "24                 24  Dermot McKeone highlighted the difference betw...   \n",
       "25                 25  As the uncritical use of text is today widely ...   \n",
       "26                 26  Neuendorf suggests that when human coders are ...   \n",
       "27                  0  In literary theory, a text is any object that ...   \n",
       "28                  1  Within the field of literary criticism, \"text\"...   \n",
       "29                  2  Since the history of writing predates the conc...   \n",
       "..                ...                                                ...   \n",
       "99                 53  The hypothesis is framed, but not asserted, in...   \n",
       "100                54  Note that the hypothesis (\"A\") could be of a r...   \n",
       "101                55  Peirce did not remain quite convinced about an...   \n",
       "102                56  In 1901 Peirce wrote, \"There would be no logic...   \n",
       "103                57  \"Consider what effects, that might conceivably...   \n",
       "104                58  It is a method for fruitful clarification of c...   \n",
       "105                59  Peirce came over the years to divide (philosop...   \n",
       "106                60  Peirce had, from the start, seen the modes of ...   \n",
       "107                61                As early as 1866, Peirce held that:   \n",
       "108                62  1. Hypothesis (abductive inference) is inferen...   \n",
       "109                63  In 1902, Peirce wrote that, in abduction: \"It ...   \n",
       "110                64  At the critical level Peirce examined the form...   \n",
       "111                65  The phrase \"inference to the best explanation\"...   \n",
       "112                66  At the methodeutical level Peirce held that a ...   \n",
       "113                67  Norwood Russell Hanson, a philosopher of scien...   \n",
       "114                68  Further development of the concept can be foun...   \n",
       "115                69  Applications in artificial intelligence includ...   \n",
       "116                70  In medicine, abduction can be seen as a compon...   \n",
       "117                71  Abduction can also be used to model automated ...   \n",
       "118                72  In intelligence analysis, analysis of competin...   \n",
       "119                73  Belief revision, the process of adapting belie...   \n",
       "120                74  In the philosophy of science, abduction has be...   \n",
       "121                75  In historical linguistics, abduction during la...   \n",
       "122                76  In anthropology, Alfred Gell in his influentia...   \n",
       "123                77  Consequently, to discover is simply to expedit...   \n",
       "124                78  It allows any flight of imagination, provided ...   \n",
       "125                79  Methodeutic has a special interest in Abductio...   \n",
       "126                80  .... What is good abduction? What should an ex...   \n",
       "127                81  The mind seeks to bring the facts, as modified...   \n",
       "128                82  Thus, twenty skillful hypotheses will ascertai...   \n",
       "\n",
       "                                                source  \\\n",
       "0       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "1       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "2       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "3       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "4       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "5       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "6       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "7       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "8       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "9       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "10      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "11      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "12      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "13      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "14      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "15      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "16      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "17      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "18      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "19      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "20      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "21      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "22      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "23      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "24      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "25      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "26      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "27   https://en.wikipedia.org/wiki/Text_(literary_t...   \n",
       "28   https://en.wikipedia.org/wiki/Text_(literary_t...   \n",
       "29   https://en.wikipedia.org/wiki/Text_(literary_t...   \n",
       "..                                                 ...   \n",
       "99   https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "100  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "101  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "102  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "103  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "104  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "105  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "106  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "107  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "108  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "109  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "110  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "111  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "112  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "113  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "114  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "115  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "116  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "117  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "118  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "119  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "120  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "121  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "122  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "123  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "124  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "125  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "126  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "127  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "128  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "\n",
       "     source-paragraph-number source-paragraph-text  \n",
       "0                        NaN                   NaN  \n",
       "1                        NaN                   NaN  \n",
       "2                        NaN                   NaN  \n",
       "3                        NaN                   NaN  \n",
       "4                        NaN                   NaN  \n",
       "5                        NaN                   NaN  \n",
       "6                        NaN                   NaN  \n",
       "7                        NaN                   NaN  \n",
       "8                        NaN                   NaN  \n",
       "9                        NaN                   NaN  \n",
       "10                       NaN                   NaN  \n",
       "11                       NaN                   NaN  \n",
       "12                       NaN                   NaN  \n",
       "13                       NaN                   NaN  \n",
       "14                       NaN                   NaN  \n",
       "15                       NaN                   NaN  \n",
       "16                       NaN                   NaN  \n",
       "17                       NaN                   NaN  \n",
       "18                       NaN                   NaN  \n",
       "19                       NaN                   NaN  \n",
       "20                       NaN                   NaN  \n",
       "21                       NaN                   NaN  \n",
       "22                       NaN                   NaN  \n",
       "23                       NaN                   NaN  \n",
       "24                       NaN                   NaN  \n",
       "25                       NaN                   NaN  \n",
       "26                       NaN                   NaN  \n",
       "27                         1                 texts  \n",
       "28                         1                 texts  \n",
       "29                         1                 texts  \n",
       "..                       ...                   ...  \n",
       "99                         2             abduction  \n",
       "100                        2             abduction  \n",
       "101                        2             abduction  \n",
       "102                        2             abduction  \n",
       "103                        2             abduction  \n",
       "104                        2             abduction  \n",
       "105                        2             abduction  \n",
       "106                        2             abduction  \n",
       "107                        2             abduction  \n",
       "108                        2             abduction  \n",
       "109                        2             abduction  \n",
       "110                        2             abduction  \n",
       "111                        2             abduction  \n",
       "112                        2             abduction  \n",
       "113                        2             abduction  \n",
       "114                        2             abduction  \n",
       "115                        2             abduction  \n",
       "116                        2             abduction  \n",
       "117                        2             abduction  \n",
       "118                        2             abduction  \n",
       "119                        2             abduction  \n",
       "120                        2             abduction  \n",
       "121                        2             abduction  \n",
       "122                        2             abduction  \n",
       "123                        2             abduction  \n",
       "124                        2             abduction  \n",
       "125                        2             abduction  \n",
       "126                        2             abduction  \n",
       "127                        2             abduction  \n",
       "128                        2             abduction  \n",
       "\n",
       "[129 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for urlTuple in otherPAgeURLS[:3]:\n",
    "    #ignore_index means the indices will not be reset after each append\n",
    "    contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tumblr API\n",
    "\n",
    "Generally website owners do not like you scraping their sites. Scraping if done badly can act like a DOS attack so you should be careful about how often you make calls to a site. Some site though want automated tools to access their data, so they create [application programming interface (APIs)](https://en.wikipedia.org/wiki/Application_programming_interface). An API specifies a procedure for an application (or script) to access their data. Often this is though a [representational state transfer (REST)](https://en.wikipedia.org/wiki/Representational_state_transfer) web service, which just means if you make correctly formatted HTTP requests they will return nicely formatted data.\n",
    "\n",
    "A nice example for us to study is [Tumblr](https://www.tumblr.com), they have a [simple RESTful API](https://www.tumblr.com/docs/en/api/v1) that allows you to read posts without any complicated html parsing.\n",
    "\n",
    "We can get the first 20 posts from a blog by making an http GET request to `'http://{blog}.tumblr.com/api/read/json'`, were `{blog}` is the name of the target blog. Lets try and get the posts from [http://lolcats-lol-cat.tumblr.com/](http://lolcats-lol-cat.tumblr.com/) (Note the blog says at the top 'One hour one pic lolcats', but the canonical name that Tumblr uses is in the URL 'lolcats-lol-cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tumblr_api_read = {\"tumblelog\":{\"title\":\"One hour one pic lolcats\",\"description\":\"\",\"name\":\"lolcats-lol-cat\",\"timezone\":\"Europe\\/Paris\",\"cname\":false,\"feeds\":[]},\"posts-start\":0,\"posts-total\":2620,\"posts-type\":false,\"posts\":[{\"id\":\"152894506281\",\"url\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/152894506281\",\"url-with-slug\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/152894506281\\/cat-kitty-selfie-mirror-staring-the-current\",\"type\":\"photo\",\"date-gmt\":\"2016-11-08 11:00:33 GMT\",\"date\":\"Tue, 08 Nov 2016 12:00:33\",\"bookmarklet\":0,\"mobile\":0,\"feed-item\":\"\",\"from-feed-id\":0,\"unix-timestamp\":1478602833,\"format\":\"html\",\"reblog-key\":\"tfr40vYd\",\"slug\":\"cat-kitty-selfie-mirror-staring-the-current\",\"is-submission\":false,\"like-button\":\"<div class=\\\"like_button\\\" data-post-id=\\\"152894506281\\\" data-blog-name=\\\"lolcats-lol-cat\\\" id=\\\"like_button_152894506281\\\"><iframe id=\\\"like_iframe_152894506281\\\" src=\\\"http:\\/\\/assets.tumblr.com\\/assets\\/html\\/like_iframe.html?_v=399aee1ea58b6007c2190a8138f100e\n"
     ]
    }
   ],
   "source": [
    "tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'))\n",
    "\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might not look very good, but it has a lot fewer angle braces than html, which is nice. What we have is [JSON](https://en.wikipedia.org/wiki/JSON) a 'human readable' text based data transmission format based on javascript. Luckily, we can convert it to a python `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['posts-start', 'posts-type', 'posts', 'posts-total', 'tumblelog'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#We need to load only the stuff between the curly braces\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "print(d.keys())\n",
    "print(len(d['posts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we read the [API specification](https://www.tumblr.com/docs/en/api/v1), we will see there are a lot of things we can get if, we add things to our GET request. First we can get posts by their id number, lets get post `146020177084`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'), params = {'id' : 146020177084})\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "d['posts'][0].keys()\n",
    "d['posts'][0]['photo-url-1280']\n",
    "\n",
    "with open('lolcat.gif', 'wb') as f:\n",
    "    gifRequest = requests.get(d['posts'][0]['photo-url-1280'], stream = True)\n",
    "    f.write(gifRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='lolcat.gif'>\n",
    "\n",
    "Such beauty, now we could get the text from all the posts as well as some metadata, like the post date, caption or the tags. But, we could instead get the links to all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>photo-type</th>\n",
       "      <th>photo-url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue, 08 Nov 2016 12:00:33</td>\n",
       "      <td>152894506281</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/b51f001944edbd78644...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, kitty, self...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed, 02 Nov 2016 22:00:21</td>\n",
       "      <td>152655732399</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/e0aaf79744b3003d16f...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, transparent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wed, 02 Nov 2016 04:00:28</td>\n",
       "      <td>152625158741</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/ce19172b18d52de1d19...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, kitty, pets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue, 01 Nov 2016 10:00:19</td>\n",
       "      <td>152590689618</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/c7b28a411454237df99...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, help, work,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sun, 30 Oct 2016 05:00:21</td>\n",
       "      <td>152493058611</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/5da6d2101c124ac5c30...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mon, 24 Oct 2016 10:00:37</td>\n",
       "      <td>152240532041</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/c9a75cca91098deb25e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fri, 21 Oct 2016 22:00:43</td>\n",
       "      <td>152125209570</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/9f7708233fcddf5d0e2...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sat, 15 Oct 2016 08:00:28</td>\n",
       "      <td>151825876698</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/5f4366cf7375a1e5677...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, illustratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sat, 08 Oct 2016 12:00:25</td>\n",
       "      <td>151508896281</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/92e6d4ec7fdeec96ff3...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sat, 08 Oct 2016 10:00:11</td>\n",
       "      <td>151506450378</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/d2358ee631f71b8d2c0...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wed, 05 Oct 2016 02:00:47</td>\n",
       "      <td>151357360495</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/6cf81365bc68cb8fd17...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Thu, 29 Sep 2016 02:00:21</td>\n",
       "      <td>151075591801</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/24b8855f6e525a0a3a1...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thu, 22 Sep 2016 10:00:18</td>\n",
       "      <td>150765069101</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/6fab060a18deec51c5d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sat, 17 Sep 2016 04:00:32</td>\n",
       "      <td>150517613333</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/fe07795651bf722048d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sat, 17 Sep 2016 02:00:40</td>\n",
       "      <td>150513586650</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/63369d156c1cf774b1a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Thu, 15 Sep 2016 14:00:37</td>\n",
       "      <td>150442786223</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/b7ea5f4df7314e9441c...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Thu, 15 Sep 2016 12:00:34</td>\n",
       "      <td>150440192230</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/f4966559ea2d202dfde...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Thu, 15 Sep 2016 10:00:26</td>\n",
       "      <td>150437927050</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/d4a588a32eea79eacae...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Thu, 08 Sep 2016 10:00:35</td>\n",
       "      <td>150113352088</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ce92c51852f7ce52259...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mon, 05 Sep 2016 12:00:21</td>\n",
       "      <td>149974124382</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/319d68989fa0082a43d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sat, 03 Sep 2016 12:00:25</td>\n",
       "      <td>149877403979</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/e3413b688a2d1349880...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wed, 31 Aug 2016 02:00:27</td>\n",
       "      <td>149721802154</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/7830189de83fb9e4fbd...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sat, 27 Aug 2016 14:00:39</td>\n",
       "      <td>149550741818</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/f6e7bf815645757a264...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wed, 24 Aug 2016 02:00:22</td>\n",
       "      <td>149390168666</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/aa2cf1aa04e595932e7...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sat, 20 Aug 2016 02:00:38</td>\n",
       "      <td>149197307846</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/a252f65bd1c8c757c80...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Thu, 18 Aug 2016 16:00:20</td>\n",
       "      <td>149127885429</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/11ccc0cb26826c934d8...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mon, 15 Aug 2016 02:00:48</td>\n",
       "      <td>148954802804</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/68e998300c29007f056...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fri, 12 Aug 2016 16:01:40</td>\n",
       "      <td>148836683947</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/89ee201721b2fb18cd1...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, animation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Fri, 12 Aug 2016 12:01:16</td>\n",
       "      <td>148830823686</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/4e7f3e90eae005779d7...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fri, 12 Aug 2016 10:01:09</td>\n",
       "      <td>148828102184</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/1de3429e06c3cd3bd2b...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Tue, 09 Aug 2016 18:00:46</td>\n",
       "      <td>148694150534</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/c551a5511d158ae35e4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Mon, 08 Aug 2016 02:00:31</td>\n",
       "      <td>148612754037</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/098a44c904cbd1e85e9...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sat, 06 Aug 2016 10:00:43</td>\n",
       "      <td>148534179811</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/3927caf5cd7fa8bde1d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Fri, 29 Jul 2016 16:00:40</td>\n",
       "      <td>148149387214</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/c078fc21432061a41ec...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sun, 24 Jul 2016 14:00:30</td>\n",
       "      <td>147890761788</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/bfb8994d1d6fcc14db1...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tue, 19 Jul 2016 04:01:09</td>\n",
       "      <td>147622173639</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/33c47f8ee7e3b339962...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, pokemon, pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Mon, 18 Jul 2016 14:01:09</td>\n",
       "      <td>147590424893</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/f9f9dc7eb93e0318981...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sun, 17 Jul 2016 06:00:25</td>\n",
       "      <td>147524647568</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/5eaae2d6e5503e01bcb...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sat, 16 Jul 2016 14:00:45</td>\n",
       "      <td>147490960707</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/51ae3d3468f8e0cfc33...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sat, 16 Jul 2016 12:00:39</td>\n",
       "      <td>147488396516</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ca7ea8f260d375e6cb9...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Thu, 14 Jul 2016 20:00:59</td>\n",
       "      <td>147403434760</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/4e9f2da9e0beab9654a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Thu, 14 Jul 2016 12:00:38</td>\n",
       "      <td>147388195456</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/f62a86855bb0ed8bf1e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tue, 12 Jul 2016 06:00:19</td>\n",
       "      <td>147272040277</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/e410bc99a005f7da2d7...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sun, 10 Jul 2016 12:00:40</td>\n",
       "      <td>147181986135</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/8c2f16d72880b674c0f...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Fri, 08 Jul 2016 08:00:25</td>\n",
       "      <td>147081244170</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/1693ef0413931b4f0ba...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Thu, 07 Jul 2016 22:01:12</td>\n",
       "      <td>147057304808</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/562befa173d729d208a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Thu, 07 Jul 2016 20:00:38</td>\n",
       "      <td>147052730234</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/d5b50cda34db2d35f72...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Sun, 03 Jul 2016 02:00:56</td>\n",
       "      <td>146821243942</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/2a133a89b7517577ae3...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sat, 02 Jul 2016 04:00:22</td>\n",
       "      <td>146779011803</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/6b34bf0bc68c08fb041...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Thu, 30 Jun 2016 14:00:55</td>\n",
       "      <td>146701076534</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/864468a24c60c78f179...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date            id photo-type  \\\n",
       "0   Tue, 08 Nov 2016 12:00:33  152894506281        gif   \n",
       "1   Wed, 02 Nov 2016 22:00:21  152655732399        gif   \n",
       "2   Wed, 02 Nov 2016 04:00:28  152625158741        gif   \n",
       "3   Tue, 01 Nov 2016 10:00:19  152590689618        gif   \n",
       "4   Sun, 30 Oct 2016 05:00:21  152493058611        jpg   \n",
       "5   Mon, 24 Oct 2016 10:00:37  152240532041        jpg   \n",
       "6   Fri, 21 Oct 2016 22:00:43  152125209570        jpg   \n",
       "7   Sat, 15 Oct 2016 08:00:28  151825876698        gif   \n",
       "8   Sat, 08 Oct 2016 12:00:25  151508896281        jpg   \n",
       "9   Sat, 08 Oct 2016 10:00:11  151506450378        jpg   \n",
       "10  Wed, 05 Oct 2016 02:00:47  151357360495        jpg   \n",
       "11  Thu, 29 Sep 2016 02:00:21  151075591801        jpg   \n",
       "12  Thu, 22 Sep 2016 10:00:18  150765069101        png   \n",
       "13  Sat, 17 Sep 2016 04:00:32  150517613333        jpg   \n",
       "14  Sat, 17 Sep 2016 02:00:40  150513586650        jpg   \n",
       "15  Thu, 15 Sep 2016 14:00:37  150442786223        jpg   \n",
       "16  Thu, 15 Sep 2016 12:00:34  150440192230        jpg   \n",
       "17  Thu, 15 Sep 2016 10:00:26  150437927050        png   \n",
       "18  Thu, 08 Sep 2016 10:00:35  150113352088        jpg   \n",
       "19  Mon, 05 Sep 2016 12:00:21  149974124382        png   \n",
       "20  Sat, 03 Sep 2016 12:00:25  149877403979        png   \n",
       "21  Wed, 31 Aug 2016 02:00:27  149721802154        png   \n",
       "22  Sat, 27 Aug 2016 14:00:39  149550741818        jpg   \n",
       "23  Wed, 24 Aug 2016 02:00:22  149390168666        jpg   \n",
       "24  Sat, 20 Aug 2016 02:00:38  149197307846        jpg   \n",
       "25  Thu, 18 Aug 2016 16:00:20  149127885429        jpg   \n",
       "26  Mon, 15 Aug 2016 02:00:48  148954802804        jpg   \n",
       "27  Fri, 12 Aug 2016 16:01:40  148836683947        gif   \n",
       "28  Fri, 12 Aug 2016 12:01:16  148830823686        jpg   \n",
       "29  Fri, 12 Aug 2016 10:01:09  148828102184        jpg   \n",
       "30  Tue, 09 Aug 2016 18:00:46  148694150534        jpg   \n",
       "31  Mon, 08 Aug 2016 02:00:31  148612754037        jpg   \n",
       "32  Sat, 06 Aug 2016 10:00:43  148534179811        png   \n",
       "33  Fri, 29 Jul 2016 16:00:40  148149387214        jpg   \n",
       "34  Sun, 24 Jul 2016 14:00:30  147890761788        jpg   \n",
       "35  Tue, 19 Jul 2016 04:01:09  147622173639        gif   \n",
       "36  Mon, 18 Jul 2016 14:01:09  147590424893        jpg   \n",
       "37  Sun, 17 Jul 2016 06:00:25  147524647568        jpg   \n",
       "38  Sat, 16 Jul 2016 14:00:45  147490960707        jpg   \n",
       "39  Sat, 16 Jul 2016 12:00:39  147488396516        jpg   \n",
       "40  Thu, 14 Jul 2016 20:00:59  147403434760        png   \n",
       "41  Thu, 14 Jul 2016 12:00:38  147388195456        png   \n",
       "42  Tue, 12 Jul 2016 06:00:19  147272040277        jpg   \n",
       "43  Sun, 10 Jul 2016 12:00:40  147181986135        jpg   \n",
       "44  Fri, 08 Jul 2016 08:00:25  147081244170        jpg   \n",
       "45  Thu, 07 Jul 2016 22:01:12  147057304808        jpg   \n",
       "46  Thu, 07 Jul 2016 20:00:38  147052730234        jpg   \n",
       "47  Sun, 03 Jul 2016 02:00:56  146821243942        jpg   \n",
       "48  Sat, 02 Jul 2016 04:00:22  146779011803        gif   \n",
       "49  Thu, 30 Jun 2016 14:00:55  146701076534        jpg   \n",
       "\n",
       "                                            photo-url  \\\n",
       "0   http://68.media.tumblr.com/b51f001944edbd78644...   \n",
       "1   http://68.media.tumblr.com/e0aaf79744b3003d16f...   \n",
       "2   http://68.media.tumblr.com/ce19172b18d52de1d19...   \n",
       "3   http://68.media.tumblr.com/c7b28a411454237df99...   \n",
       "4   http://68.media.tumblr.com/5da6d2101c124ac5c30...   \n",
       "5   http://68.media.tumblr.com/c9a75cca91098deb25e...   \n",
       "6   http://68.media.tumblr.com/9f7708233fcddf5d0e2...   \n",
       "7   http://68.media.tumblr.com/5f4366cf7375a1e5677...   \n",
       "8   http://68.media.tumblr.com/92e6d4ec7fdeec96ff3...   \n",
       "9   http://68.media.tumblr.com/d2358ee631f71b8d2c0...   \n",
       "10  http://68.media.tumblr.com/6cf81365bc68cb8fd17...   \n",
       "11  http://68.media.tumblr.com/24b8855f6e525a0a3a1...   \n",
       "12  http://68.media.tumblr.com/6fab060a18deec51c5d...   \n",
       "13  http://68.media.tumblr.com/fe07795651bf722048d...   \n",
       "14  http://68.media.tumblr.com/63369d156c1cf774b1a...   \n",
       "15  http://68.media.tumblr.com/b7ea5f4df7314e9441c...   \n",
       "16  http://68.media.tumblr.com/f4966559ea2d202dfde...   \n",
       "17  http://68.media.tumblr.com/d4a588a32eea79eacae...   \n",
       "18  http://68.media.tumblr.com/ce92c51852f7ce52259...   \n",
       "19  http://68.media.tumblr.com/319d68989fa0082a43d...   \n",
       "20  http://68.media.tumblr.com/e3413b688a2d1349880...   \n",
       "21  http://68.media.tumblr.com/7830189de83fb9e4fbd...   \n",
       "22  http://68.media.tumblr.com/f6e7bf815645757a264...   \n",
       "23  http://68.media.tumblr.com/aa2cf1aa04e595932e7...   \n",
       "24  http://68.media.tumblr.com/a252f65bd1c8c757c80...   \n",
       "25  http://68.media.tumblr.com/11ccc0cb26826c934d8...   \n",
       "26  http://68.media.tumblr.com/68e998300c29007f056...   \n",
       "27  http://68.media.tumblr.com/89ee201721b2fb18cd1...   \n",
       "28  http://68.media.tumblr.com/4e7f3e90eae005779d7...   \n",
       "29  http://68.media.tumblr.com/1de3429e06c3cd3bd2b...   \n",
       "30  http://68.media.tumblr.com/c551a5511d158ae35e4...   \n",
       "31  http://68.media.tumblr.com/098a44c904cbd1e85e9...   \n",
       "32  http://68.media.tumblr.com/3927caf5cd7fa8bde1d...   \n",
       "33  http://68.media.tumblr.com/c078fc21432061a41ec...   \n",
       "34  http://68.media.tumblr.com/bfb8994d1d6fcc14db1...   \n",
       "35  http://68.media.tumblr.com/33c47f8ee7e3b339962...   \n",
       "36  http://68.media.tumblr.com/f9f9dc7eb93e0318981...   \n",
       "37  http://68.media.tumblr.com/5eaae2d6e5503e01bcb...   \n",
       "38  http://68.media.tumblr.com/51ae3d3468f8e0cfc33...   \n",
       "39  http://68.media.tumblr.com/ca7ea8f260d375e6cb9...   \n",
       "40  http://68.media.tumblr.com/4e9f2da9e0beab9654a...   \n",
       "41  http://68.media.tumblr.com/f62a86855bb0ed8bf1e...   \n",
       "42  http://68.media.tumblr.com/e410bc99a005f7da2d7...   \n",
       "43  http://68.media.tumblr.com/8c2f16d72880b674c0f...   \n",
       "44  http://68.media.tumblr.com/1693ef0413931b4f0ba...   \n",
       "45  http://68.media.tumblr.com/562befa173d729d208a...   \n",
       "46  http://68.media.tumblr.com/d5b50cda34db2d35f72...   \n",
       "47  http://68.media.tumblr.com/2a133a89b7517577ae3...   \n",
       "48  http://68.media.tumblr.com/6b34bf0bc68c08fb041...   \n",
       "49  http://68.media.tumblr.com/864468a24c60c78f179...   \n",
       "\n",
       "                                                 tags  \n",
       "0   [gif, lolcat, lolcats, cat, funny, kitty, self...  \n",
       "1   [gif, lolcat, lolcats, cat, funny, transparent...  \n",
       "2   [gif, lolcat, lolcats, cat, funny, kitty, pets...  \n",
       "3   [gif, lolcat, lolcats, cat, funny, help, work,...  \n",
       "4                   [cat, cats, lol, lolcat, lolcats]  \n",
       "5                   [cat, cats, lol, lolcat, lolcats]  \n",
       "6                   [cat, cats, lol, lolcat, lolcats]  \n",
       "7   [gif, lolcat, lolcats, cat, funny, illustratio...  \n",
       "8                   [cat, cats, lol, lolcat, lolcats]  \n",
       "9                   [cat, cats, lol, lolcat, lolcats]  \n",
       "10                  [cat, cats, lol, lolcat, lolcats]  \n",
       "11                  [cat, cats, lol, lolcat, lolcats]  \n",
       "12                  [cat, cats, lol, lolcat, lolcats]  \n",
       "13                  [cat, cats, lol, lolcat, lolcats]  \n",
       "14                  [cat, cats, lol, lolcat, lolcats]  \n",
       "15                  [cat, cats, lol, lolcat, lolcats]  \n",
       "16                  [cat, cats, lol, lolcat, lolcats]  \n",
       "17                  [cat, cats, lol, lolcat, lolcats]  \n",
       "18                  [cat, cats, lol, lolcat, lolcats]  \n",
       "19                  [cat, cats, lol, lolcat, lolcats]  \n",
       "20                  [cat, cats, lol, lolcat, lolcats]  \n",
       "21                  [cat, cats, lol, lolcat, lolcats]  \n",
       "22                  [cat, cats, lol, lolcat, lolcats]  \n",
       "23                  [cat, cats, lol, lolcat, lolcats]  \n",
       "24                  [cat, cats, lol, lolcat, lolcats]  \n",
       "25                  [cat, cats, lol, lolcat, lolcats]  \n",
       "26                  [cat, cats, lol, lolcat, lolcats]  \n",
       "27  [gif, lolcat, lolcats, cat, funny, animation, ...  \n",
       "28                  [cat, cats, lol, lolcat, lolcats]  \n",
       "29                  [cat, cats, lol, lolcat, lolcats]  \n",
       "30                  [cat, cats, lol, lolcat, lolcats]  \n",
       "31                  [cat, cats, lol, lolcat, lolcats]  \n",
       "32                  [cat, cats, lol, lolcat, lolcats]  \n",
       "33                  [cat, cats, lol, lolcat, lolcats]  \n",
       "34                  [cat, cats, lol, lolcat, lolcats]  \n",
       "35  [gif, lolcat, lolcats, cat, funny, pokemon, pi...  \n",
       "36                  [cat, cats, lol, lolcat, lolcats]  \n",
       "37                  [cat, cats, lol, lolcat, lolcats]  \n",
       "38                  [cat, cats, lol, lolcat, lolcats]  \n",
       "39                  [cat, cats, lol, lolcat, lolcats]  \n",
       "40                  [cat, cats, lol, lolcat, lolcats]  \n",
       "41                  [cat, cats, lol, lolcat, lolcats]  \n",
       "42                  [cat, cats, lol, lolcat, lolcats]  \n",
       "43                  [cat, cats, lol, lolcat, lolcats]  \n",
       "44                  [cat, cats, lol, lolcat, lolcats]  \n",
       "45                  [cat, cats, lol, lolcat, lolcats]  \n",
       "46                  [cat, cats, lol, lolcat, lolcats]  \n",
       "47                  [cat, cats, lol, lolcat, lolcats]  \n",
       "48                 [gif, lolcat, lolcats, cat, funny]  \n",
       "49                  [cat, cats, lol, lolcat, lolcats]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting a max incase the blog has millions of images\n",
    "#The given max will be rounded up to the nearest multiple of 50\n",
    "def tumblrImageScrape(blogName, maxImages = 200):\n",
    "    #Restating this here so the function isn't dependent on any external variables\n",
    "    tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "    #There are a bunch of possible locations for the photo url\n",
    "    possiblePhotoSuffixes = [1280, 500, 400, 250, 100]\n",
    "\n",
    "    #These are the pieces of information we will be gathering,\n",
    "    #at the end we will convert this to a DataFrame.\n",
    "    #There are a few other ones we could get like the captions\n",
    "    #you can read the Tumblr documentation to learn how to get them\n",
    "    #https://www.tumblr.com/docs/en/api/v1\n",
    "    postsData = {\n",
    "        'id' : [],\n",
    "        'photo-url' : [],\n",
    "        'date' : [],\n",
    "        'tags' : [],\n",
    "        'photo-type' : []\n",
    "    }\n",
    "\n",
    "    #Tumblr limits us to a max of 50 posts per request\n",
    "    for requestNum in range(maxImages // 50):\n",
    "        requestParams = {\n",
    "            'start' : requestNum * 50,\n",
    "            'num' : 50,\n",
    "            'type' : 'photo'\n",
    "        }\n",
    "        r = requests.get(tumblrAPItarget.format(blogName), params = requestParams)\n",
    "        requestDict = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "        for postDict in requestDict['posts']:\n",
    "            #We are dealing with uncleaned data, we can't trust it.\n",
    "            #Specifically, not all posts are guaranteed to have the fields we want\n",
    "            try:\n",
    "                postsData['id'].append(postDict['id'])\n",
    "                postsData['date'].append(postDict['date'])\n",
    "                postsData['tags'].append(postDict['tags'])\n",
    "            except KeyError as e:\n",
    "                raise KeyError(\"Post {} from {} is missing: {}\".format(postDict['id'], blogName, e))\n",
    "\n",
    "            foundSuffix = False\n",
    "            for suffix in possiblePhotoSuffixes:\n",
    "                try:\n",
    "                    photoURL = postDict['photo-url-{}'.format(suffix)]\n",
    "                    postsData['photo-url'].append(photoURL)\n",
    "                    postsData['photo-type'].append(photoURL.split('.')[-1])\n",
    "                    foundSuffix = True\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if not foundSuffix:\n",
    "                #Make sure your error messages are useful\n",
    "                #You will be one of the users\n",
    "                raise KeyError(\"Post {} from {} is missing a photo url\".format(postDict['id'], blogName))\n",
    "\n",
    "    return pandas.DataFrame(postsData)\n",
    "tumblrImageScrape('lolcats-lol-cat', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the urls of a bunch of images and can run OCR on them.\n",
    "\n",
    "## Stuff for the v2 Tumblr API\n",
    "\n",
    "probably unnecessary\n",
    "\n",
    "+ Consumer Key: TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8\n",
    "+ Secret Key:  GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv\n",
    "\n",
    "[https://www.tumblr.com/docs/en/api/v2](https://www.tumblr.com/docs/en/api/v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the most recent 20 posts\n",
    "target = 'http://procedural-generation.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(target)\n",
    "\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "\n",
    "#get a specific post\n",
    "\n",
    "r = requests.get(target, params = {'id' : '152256405098'})\n",
    "\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nREQUEST_TOKEN_URL = \\'http://www.tumblr.com/oauth/request_token\\'\\nAUTHORIZATION_URL = \\'http://www.tumblr.com/oauth/authorize\\'\\nACCESS_TOKEN_URL = \\'http://www.tumblr.com/oauth/access_token\\'\\n\\ntumblrKey = \\'TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8\\'\\ntumblrSecret = \\'GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv\\'\\nconsumer = oauth2.Consumer(tumblrKey, tumblrSecret)\\nclient = oauth2.Client(consumer)\\n\\n#The token is part of content, which is provided as a binary query string\\nresponseDict, content = client.request(REQUEST_TOKEN_URL, \"GET\")\\ncontent = urllib.parse.parse_qs(content.decode(\\'utf-8\\'))\\noathToken = content[\\'oauth_token\\'][0]\\noathTokenSecret = content[\\'oauth_token_secret\\'][0]\\n\\nprint(\"The OATH token is: {}\\nThe OATH token secret is: {}\".format(oathToken, oathTokenSecret))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "REQUEST_TOKEN_URL = 'http://www.tumblr.com/oauth/request_token'\n",
    "AUTHORIZATION_URL = 'http://www.tumblr.com/oauth/authorize'\n",
    "ACCESS_TOKEN_URL = 'http://www.tumblr.com/oauth/access_token'\n",
    "\n",
    "tumblrKey = 'TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8'\n",
    "tumblrSecret = 'GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv'\n",
    "consumer = oauth2.Consumer(tumblrKey, tumblrSecret)\n",
    "client = oauth2.Client(consumer)\n",
    "\n",
    "#The token is part of content, which is provided as a binary query string\n",
    "responseDict, content = client.request(REQUEST_TOKEN_URL, \"GET\")\n",
    "content = urllib.parse.parse_qs(content.decode('utf-8'))\n",
    "oathToken = content['oauth_token'][0]\n",
    "oathTokenSecret = content['oauth_token_secret'][0]\n",
    "\n",
    "print(\"The OATH token is: {}\\nThe OATH token secret is: {}\".format(oathToken, oathTokenSecret))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n",
    "Something about subprocess\n",
    "\n",
    "`pytesseract` works but requires tesseract binary\n",
    "\n",
    "# Files\n",
    "\n",
    "What if the text we want isn't on a webpage? There are a many other sources of text available.\n",
    "\n",
    "## Raw text\n",
    "\n",
    "The most basic form of storing text is as a _raw text_ document. Source code (`.py`, `.r`, etc) is usually raw text as are text files (`.txt`) and many other things. Opening an unknown file with a text editor is often a great way of learning what the file is.\n",
    "\n",
    "We can create a text file with the `open()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_text_file = 'sometextfile.txt'\n",
    "#stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols \\u2421 \\u241B \\u20A0 \\u20A1 \\u20A2 \\u20A3 \\u0D60\\n'\n",
    "stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\\n'\n",
    "\n",
    "with open(example_text_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(stringToWrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice though the `encoding='utf-8'` argument, the encoding specifies how we map the bits from the file to the glyphs (and whitespace characters like tab (`'\\t'`) or newline (`'\\n'`)) on the screen. When dealing only with latin letters, arabic numerals and the other symbols on America keyboards you usually do not have to worry about encodings as the ones used today are backwards compatible with [ASCII](https://en.wikipedia.org/wiki/ASCII) which gives the binary representation of 128 characters.\n",
    "\n",
    "Some people though use other characters. To solve this there is [Unicode](https://en.wikipedia.org/wiki/Unicode) which gives numbers to symbols, e.g. 041 is `'A'` and 03A3 is `'Σ'` (number starting with 0 indicates they are hexadecimal), often non-ASCII characters are called Unicode characters. Unfortunately there are many ways used to map combinations of bits to Unicode symbols. The ones you are likely to encounter are called by Python _utf-8_, _utf-16_ and _latin-1_. _utf-8_ is the standard for Linux and Mac OS while both _utf-16_ and _latin-1_ are used by windows. If you use the wrong encoding characters can appear wrong, sometimes change in number or Python could raise an exception. Lets see what happens when we open the file we just created with different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with the correct encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\n",
      "\n",
      "This is with the wrong encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols â¡ â â  â¡ â¢ â£ àµ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(example_text_file, encoding='utf-8') as f:\n",
    "    print(\"This is with the correct encoding:\")\n",
    "    print(f.read())\n",
    "\n",
    "with open(example_text_file, encoding='latin-1') as f:\n",
    "    print(\"This is with the wrong encoding:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with _latin-1_ the unicode characters are mixed up and there are too many of them. You need to keep in mind encoding when obtaining text files, as determining the encoding can sometime be a lot of work.\n",
    "\n",
    "## PDF\n",
    "\n",
    "Another common way text will be stored is in a PDF file. First we will download a pdf in Python. To do that lets grab a chapter from\n",
    "_Speech and Language Processing_, chapter 20 is on Information Extraction which seems apt. It is stored as a pdf at [https://web.stanford.edu/~jurafsky/slp3/20.pdf](https://web.stanford.edu/~jurafsky/slp3/20.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "<title>Object not found! : Stanford University</title>\n",
      "<script type=\"text/javascript\">\n",
      "<!--\n",
      "if ((window.screen.width < 640) || (window.screen.height < 640)){document.write('<meta name=\"viewport\" content=\"width=device-width, user-scalable=yes, initial-scale=1\">')}\n",
      "//-->\n",
      "</script>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
      "<link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"//www.stanford.edu/favicon.ico\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//www.stanford.edu/su-identity/css/su-identity.css\" media=\"all\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700\" media=\"all\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//www.stanford.edu/stanfordmodern/v2.5/css/stanfordmodern.css\" media=\"all\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"//www.stanford.edu/stanfordmodern/v2.5/css/mobile.css\" media=\"only screen and (max-width: 640px)\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" hr\n"
     ]
    }
   ],
   "source": [
    "#information_extraction_pdf = 'https://web.stanford.edu/~jurafsky/slp3/20.pdf'\n",
    "\n",
    "infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)\n",
    "print(infoExtractionRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says `'pdf'`, so thats a good sign, the rest though looks like we are having issues with an encoding. The random characters are not though caused by our encoding being wrong they are cause by there not being an encoding for those parts at all. PDFs are nominally binary files, meaning there are sections of binary that are specific to pdf and nothing else so you need something that knows about pdf to read them. To do that we will be using [`PyPDF2`](https://github.com/mstamy2/PyPDF2) which is a PDF processing library for Python 3.\n",
    "\n",
    "**NOTE** maybe use `PyPDF2` or `slate`\n",
    "\n",
    "Because PDFs are a very complicated file format pdfminer requires a large amount of boilerplate code to extract text, we have written a function that takes in an open PDF file and returns the text so you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    #Make utf-8 explicit\n",
    "    #Based on code from http://stackoverflow.com/a/20905381/4955164\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to take the response object and convert it into a 'file like' object so that pdfminer can read it. To do this we will use `io`'s `BytesIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoExtractionBytes = io.BytesIO(infoExtractionRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can give it to pdfminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFSyntaxError",
     "evalue": "No /Root object! - Is this really a PDF?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPDFSyntaxError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-221f225c5a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadPDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfoExtractionBytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-f09fc10f0ecd>\u001b[0m in \u001b[0;36mreadPDF\u001b[0;34m(pdfFile)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpagenos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPDFPage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpagenos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_extractable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/pdfminer/pdfpage.py\u001b[0m in \u001b[0;36mget_pages\u001b[0;34m(klass, fp, pagenos, maxpages, password, caching, check_extractable)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Create a PDF document object that stores the document structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Check if the document allows text extraction. If not, abort.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_extractable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_extractable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/pdfminer/pdfdocument.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPDFSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No /Root object! - Is this really a PDF?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mLITERAL_CATALOG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPDFSyntaxError\u001b[0m: No /Root object! - Is this really a PDF?"
     ]
    }
   ],
   "source": [
    "print(readPDF(infoExtractionBytes)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can either look at the full text or fiddle with our PDF reader and get more information about individual blocks of text.\n",
    "\n",
    "## Word Docs\n",
    "\n",
    "*NOTE* The package is called python-docx\n",
    "\n",
    "The other type of document you are likely to encounter is the `.docx`, these are actually a version of [XML](https://en.wikipedia.org/wiki/Office_Open_XML), just like HTML, and like HTML we will use a specialized parser.\n",
    "\n",
    "For this class we will use [`python-docx`](https://python-docx.readthedocs.io/en/latest/) which provides a nice simple interface for reading `.docx` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wenxi Xiao\n",
      "Professor Benjamin Soltoff\n",
      "MACS 30000\n",
      "10 October 2016\n",
      "Ethics of “Taste, Ties, and Time”\n",
      "\tIn 2008, sociologist Kevin Lewis and colleagues conduced a research project, entitled “Taste, Ties, and Time (3T),” to examine how race and cultural tastes could affect people’s social relationships. Researchers collected 1,640 college students’ Facebook profiles from Facebook.com and combined these data with the students’ school records, creating a new social network dataset. Through subsequent quantitative analyses of the dataset researchers concluded that gender, race, and socioeconomic status all influenced how these students behaved in social networks. In addition, the results showed that students’ Facebook friendships were correlated with their cultural preferences. Lewis and colleagues’ findings were published in the journal Social Networks and opened up a new branch in social science research. Despite its scientific potential, the project attracted critics from the public as well as the scientific community. The researchers were accused of violating research ethics, such that they scraped data without gaining informed consent and invading the students’ personal privacy by releasing the dataset to other researchers (Zimmer, 2010). Consequently, the students in the 3T project were identified, resulting a withdrawal of the dataset (Salganik, in open review). In this essay, I evaluate how the 3T project adheres to Salganik's four principles of ethical research, which are Respect for Persons, Beneficence, Justice, and Respect for Law and Public Interest.\n",
      "\tFirst of all, the 3T project violates the first principle, Respect for Persons, which is to ensure research participants’ autonomy (Salganik, in open review). According to the principle of Respect for Persons, people are entitled to be on their own free will to decide whether or not to participate in a given study. Additionally, special population such as children and prisoners are required to be treated with additional cautions. Researchers implement Respect for Persons by obtaining participants’ informed consent, a comprehensive written document allowing participants to be aware of the study’s procedures and potential risks. In the 3T project, researchers used students’ Facebook profiles data without the students’ awareness, not to mention obtaining informed consent. Such conduct clearly disobeyed the Respect for Persons principle. Although Lewis and colleagues claimed that when people registered for a Facebook account they agreed to allow Facebook to use their online data for research purposes by checking the term of use, it is still the researchers’ responsibility to explicitly give the students an opportunity to choose if they wanted their data to be included. After all, not many people carefully read all the items in term of use. Admittedly, violating the principle of Respect for Persons does not doom a study to a forbidden fate, but rather it alerts the researchers that their study needs amendments for being a more ethical one. \n"
     ]
    }
   ],
   "source": [
    "r = requests.get('https://github.com/xiaow2/persp-analysis/raw/02772bc5baf4044ba6410170ca740f14cd6155d5/assignments/short%20paper%201.docx', stream=True)\n",
    "d = docx.Document(io.BytesIO(r.content))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
